{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "basic-mercy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:04.318010Z",
     "start_time": "2021-05-07T11:23:03.421452Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import importlib\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import grb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-stable",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dense-stocks",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "australian-miami",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:04.594466Z",
     "start_time": "2021-05-07T11:23:04.319907Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"/data/qinkai/aminer_revised/\"\n",
    "with open(os.path.join(data_dir, \"adj.pkl\"), 'rb') as f:\n",
    "    adj = pickle.load(f)\n",
    "labels = np.load(os.path.join(data_dir, \"labels_train.npy\"))\n",
    "features = np.load(os.path.join(data_dir, \"features.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "german-shoulder",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:04.603757Z",
     "start_time": "2021-05-07T11:23:04.596984Z"
    }
   },
   "outputs": [],
   "source": [
    "# padding unknown test labels by zeros\n",
    "# labels = np.concatenate([labels, np.zeros(features.shape[0] - labels.shape[0])], axis=0)\n",
    "\n",
    "# padding unknown test labels by true labels\n",
    "labels_test = np.load(os.path.join(data_dir, \"labels_test.npy\"))\n",
    "labels = np.concatenate([labels, labels_test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-syndrome",
   "metadata": {},
   "source": [
    "## Build customized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interesting-slide",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:06.187600Z",
     "start_time": "2021-05-07T11:23:04.610246Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.dataset.dataset import CustomDataset\n",
    "from grb.utils import fix_seed, get_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "natural-crest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:06.366002Z",
     "start_time": "2021-05-07T11:23:06.191762Z"
    }
   },
   "outputs": [],
   "source": [
    "n_node = features.shape[0]\n",
    "n_val = 50000  # user-defined val size\n",
    "n_test = 50000\n",
    "train_mask = torch.zeros(n_node, dtype=bool)\n",
    "train_mask[range(n_node - n_val - n_test)] = True\n",
    "val_mask = torch.zeros(n_node, dtype=bool)\n",
    "val_mask[range(n_node - n_val - n_test, n_node - n_test)] = True\n",
    "test_mask = torch.zeros(n_node, dtype=bool)\n",
    "test_mask[range(n_node - n_test, n_node)] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "possible-damage",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:06.558581Z",
     "start_time": "2021-05-07T11:23:06.367522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Dataset 'Aminer' loaded.\n",
      "    Number of nodes: 659574.\n",
      "    Number of edges: 5757154.\n",
      "    Number of features: 100.\n",
      "    Number of classes: 18.\n",
      "    Number of train samples: 559574.\n",
      "    Number of val samples: 50000.\n",
      "    Number of test samples: 50000.\n",
      "    Feature range [-0.6672, 0.6483]\n"
     ]
    }
   ],
   "source": [
    "dataset = CustomDataset(adj=adj,\n",
    "                        features=features,\n",
    "                        labels=labels,\n",
    "                        train_mask=train_mask,\n",
    "                        val_mask=val_mask,\n",
    "                        test_mask=test_mask,\n",
    "                        name='Aminer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "academic-payday",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:06.587609Z",
     "start_time": "2021-05-07T11:23:06.581007Z"
    }
   },
   "outputs": [],
   "source": [
    "adj = dataset.adj\n",
    "adj_tensor = dataset.adj_tensor\n",
    "features = dataset.features\n",
    "labels = dataset.labels\n",
    "num_features = dataset.num_features\n",
    "num_classes = dataset.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc388f",
   "metadata": {},
   "source": [
    "## Fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "separate-bullet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T08:20:27.451054Z",
     "start_time": "2021-05-06T08:20:27.443763Z"
    }
   },
   "outputs": [],
   "source": [
    "# fix_seed(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-medicaid",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-blond",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-mississippi",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "center-contents",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:20.527349Z",
     "start_time": "2021-05-07T11:23:20.508209Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 68178.\n",
      "GCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): GCNConv(\n",
      "      (linear): Linear(in_features=100, out_features=256, bias=True)\n",
      "    )\n",
      "    (1): GCNConv(\n",
      "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): GCNConv(\n",
      "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (3): GCNConv(\n",
      "      (linear): Linear(in_features=64, out_features=18, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.gcn import GCN\n",
    "\n",
    "model = GCN(in_features=num_features, \n",
    "            out_features=num_classes, \n",
    "            hidden_features=[256, 128, 64], \n",
    "            activation=F.relu)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab41066",
   "metadata": {},
   "source": [
    "### Build model (with LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad989f67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:56:46.197340Z",
     "start_time": "2021-05-06T10:56:46.181603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 69146.\n",
      "GCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): GCNConv(\n",
      "      (linear): Linear(in_features=100, out_features=256, bias=True)\n",
      "    )\n",
      "    (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): GCNConv(\n",
      "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (5): GCNConv(\n",
      "      (linear): Linear(in_features=128, out_features=64, bias=True)\n",
      "    )\n",
      "    (6): GCNConv(\n",
      "      (linear): Linear(in_features=64, out_features=18, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.gcn import GCN\n",
    "\n",
    "model = GCN(in_features=num_features, \n",
    "            out_features=num_classes, \n",
    "            hidden_features=[256, 128, 64], \n",
    "            layer_norm=True,\n",
    "            activation=F.relu)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-complaint",
   "metadata": {},
   "source": [
    "### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "earlier-navigator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T11:23:32.742897Z",
     "start_time": "2021-05-07T11:23:26.798517Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.model.trainer import Trainer\n",
    "from grb.utils.normalize import GCNAdjNorm\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(dataset=dataset, \n",
    "                  optimizer=adam, \n",
    "                  loss=F.nll_loss,\n",
    "                  adj_norm_func=GCNAdjNorm,\n",
    "                  lr_scheduler=False,\n",
    "                  early_stop=False,\n",
    "                  device='cuda:4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-tourism",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-suggestion",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-07T11:23:40.235Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 2.8837 | Train Acc 0.0239 | Val Loss 2.8838 | Val Acc 0.0237\n",
      "Epoch 00100 | Train Loss 1.3392 | Train Acc 0.5981 | Val Loss 1.3529 | Val Acc 0.5931\n",
      "Epoch 00200 | Train Loss 1.2148 | Train Acc 0.6283 | Val Loss 1.2258 | Val Acc 0.6230\n",
      "Epoch 00300 | Train Loss 1.1707 | Train Acc 0.6384 | Val Loss 1.1849 | Val Acc 0.6341\n",
      "Epoch 00400 | Train Loss 1.1412 | Train Acc 0.6465 | Val Loss 1.1577 | Val Acc 0.6417\n",
      "Epoch 00500 | Train Loss 1.1259 | Train Acc 0.6504 | Val Loss 1.1466 | Val Acc 0.6428\n",
      "Epoch 00600 | Train Loss 1.1106 | Train Acc 0.6535 | Val Loss 1.1321 | Val Acc 0.6478\n",
      "Epoch 00700 | Train Loss 1.1017 | Train Acc 0.6563 | Val Loss 1.1245 | Val Acc 0.6508\n",
      "Epoch 00800 | Train Loss 1.0945 | Train Acc 0.6575 | Val Loss 1.1214 | Val Acc 0.6502\n",
      "Epoch 00900 | Train Loss 1.0876 | Train Acc 0.6588 | Val Loss 1.1120 | Val Acc 0.6535\n",
      "Epoch 01000 | Train Loss 1.0803 | Train Acc 0.6616 | Val Loss 1.1071 | Val Acc 0.6530\n",
      "Best validation accuracy: 0.6554\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 01100 | Train Loss 1.0767 | Train Acc 0.6620 | Val Loss 1.1036 | Val Acc 0.6554\n",
      "Epoch 01200 | Train Loss 1.0744 | Train Acc 0.6629 | Val Loss 1.1022 | Val Acc 0.6554\n",
      "Epoch 01300 | Train Loss 1.0710 | Train Acc 0.6634 | Val Loss 1.1009 | Val Acc 0.6547\n",
      "Best validation accuracy: 0.6558\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 01400 | Train Loss 1.0698 | Train Acc 0.6633 | Val Loss 1.0982 | Val Acc 0.6558\n",
      "Best validation accuracy: 0.6589\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 01500 | Train Loss 1.0644 | Train Acc 0.6655 | Val Loss 1.0943 | Val Acc 0.6589\n",
      "Epoch 01600 | Train Loss 1.0603 | Train Acc 0.6661 | Val Loss 1.0937 | Val Acc 0.6568\n",
      "Epoch 01700 | Train Loss 1.0611 | Train Acc 0.6666 | Val Loss 1.0923 | Val Acc 0.6568\n",
      "Epoch 01800 | Train Loss 1.0561 | Train Acc 0.6678 | Val Loss 1.0886 | Val Acc 0.6580\n",
      "Epoch 01900 | Train Loss 1.0572 | Train Acc 0.6666 | Val Loss 1.0921 | Val Acc 0.6576\n",
      "Best validation accuracy: 0.6594\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 02000 | Train Loss 1.0525 | Train Acc 0.6684 | Val Loss 1.0852 | Val Acc 0.6594\n",
      "Best validation accuracy: 0.6600\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 02100 | Train Loss 1.0540 | Train Acc 0.6687 | Val Loss 1.0885 | Val Acc 0.6600\n",
      "Epoch 02200 | Train Loss 1.0515 | Train Acc 0.6681 | Val Loss 1.0845 | Val Acc 0.6594\n",
      "Best validation accuracy: 0.6603\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 02300 | Train Loss 1.0497 | Train Acc 0.6691 | Val Loss 1.0861 | Val Acc 0.6603\n",
      "Epoch 02400 | Train Loss 1.0505 | Train Acc 0.6690 | Val Loss 1.0850 | Val Acc 0.6595\n",
      "Epoch 02500 | Train Loss 1.0491 | Train Acc 0.6692 | Val Loss 1.0831 | Val Acc 0.6600\n",
      "Best validation accuracy: 0.6626\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 02600 | Train Loss 1.0451 | Train Acc 0.6706 | Val Loss 1.0820 | Val Acc 0.6626\n",
      "Epoch 02700 | Train Loss 1.0444 | Train Acc 0.6709 | Val Loss 1.0819 | Val Acc 0.6625\n",
      "Epoch 02800 | Train Loss 1.0450 | Train Acc 0.6707 | Val Loss 1.0798 | Val Acc 0.6616\n",
      "Epoch 02900 | Train Loss 1.0459 | Train Acc 0.6703 | Val Loss 1.0839 | Val Acc 0.6604\n",
      "Epoch 03000 | Train Loss 1.0416 | Train Acc 0.6718 | Val Loss 1.0781 | Val Acc 0.6618\n",
      "Epoch 03100 | Train Loss 1.0430 | Train Acc 0.6709 | Val Loss 1.0809 | Val Acc 0.6624\n",
      "Epoch 03200 | Train Loss 1.0418 | Train Acc 0.6711 | Val Loss 1.0766 | Val Acc 0.6617\n",
      "Epoch 03300 | Train Loss 1.0422 | Train Acc 0.6711 | Val Loss 1.0789 | Val Acc 0.6624\n",
      "Epoch 03400 | Train Loss 1.0409 | Train Acc 0.6711 | Val Loss 1.0800 | Val Acc 0.6615\n",
      "Best validation accuracy: 0.6631\n",
      "Model saved in './saved_models/gcn_aminer/checkpoint.pt'.\n",
      "Epoch 03500 | Train Loss 1.0406 | Train Acc 0.6721 | Val Loss 1.0795 | Val Acc 0.6631\n",
      "Epoch 03600 | Train Loss 1.0395 | Train Acc 0.6722 | Val Loss 1.0772 | Val Acc 0.6630\n",
      "Epoch 03700 | Train Loss 1.0380 | Train Acc 0.6719 | Val Loss 1.0757 | Val Acc 0.6628\n",
      "Epoch 03800 | Train Loss 1.0381 | Train Acc 0.6719 | Val Loss 1.0799 | Val Acc 0.6617\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model=model,\n",
    "              n_epoch=12000,\n",
    "              save_dir=\"./saved_models/gcn_aminer\",\n",
    "              eval_every=100,\n",
    "              save_after=1000,\n",
    "              dropout=0.5,\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-auckland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T05:55:03.150973Z",
     "start_time": "2021-04-23T05:55:03.127334Z"
    }
   },
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "gross-puzzle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:57:18.672267Z",
     "start_time": "2021-05-06T10:57:15.084901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved_models/gcn_ln_aminer/checkpoint_final.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tamil-stake",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:57:19.514620Z",
     "start_time": "2021-05-06T10:57:18.674292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.6835\n"
     ]
    }
   ],
   "source": [
    "logits, test_acc = trainer.inference(model)\n",
    "print(\"Test ACC: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-firmware",
   "metadata": {},
   "source": [
    "## TAGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-directory",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "copyrighted-arrow",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T00:51:18.697309Z",
     "start_time": "2021-05-06T00:51:18.690457Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.utils import fix_seed, get_num_params\n",
    "\n",
    "fix_seed(seed=0)  # fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "united-controversy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:30:31.680232Z",
     "start_time": "2021-05-06T10:30:31.662269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 144018.\n",
      "TAGCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): TAGConv(\n",
      "      (linear): Linear(in_features=300, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): TAGConv(\n",
      "      (linear): Linear(in_features=384, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): TAGConv(\n",
      "      (linear): Linear(in_features=384, out_features=128, bias=True)\n",
      "    )\n",
      "    (3): TAGConv(\n",
      "      (linear): Linear(in_features=384, out_features=18, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.tagcn import TAGCN\n",
    "\n",
    "model = TAGCN(in_features=num_features, out_features=num_classes, hidden_features=[128, 128, 128], k=2, \n",
    "              activation=F.leaky_relu)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-witch",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T06:43:59.949101Z",
     "start_time": "2021-04-23T06:43:59.944619Z"
    }
   },
   "source": [
    "### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "healthy-omaha",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:30:45.236402Z",
     "start_time": "2021-05-06T10:30:36.425548Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.model.trainer import Trainer\n",
    "from grb.utils.normalize import GCNAdjNorm\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "trainer = Trainer(dataset=dataset, \n",
    "                  optimizer=adam, \n",
    "                  loss=F.nll_loss,\n",
    "                  adj_norm_func=GCNAdjNorm,\n",
    "                  lr_scheduler=True,\n",
    "                  early_stop=True,\n",
    "                  device='cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-heath",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "remarkable-paint",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:15:23.323046Z",
     "start_time": "2021-05-06T00:51:41.365605Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 2.9431 | Train Acc 0.0580 | Val Loss 2.9444 | Val Acc 0.0577\n",
      "Model saved in './saved_models/tagcn_aminer/checkpoint_epoch_0.pt'.\n",
      "Epoch 00200 | Train Loss 1.2170 | Train Acc 0.6289 | Val Loss 1.2292 | Val Acc 0.6246\n",
      "Epoch 00400 | Train Loss 1.1139 | Train Acc 0.6540 | Val Loss 1.1294 | Val Acc 0.6488\n",
      "Model saved in './saved_models/tagcn_aminer/checkpoint_epoch_500.pt'.\n",
      "Epoch 00600 | Train Loss 1.0611 | Train Acc 0.6663 | Val Loss 1.0770 | Val Acc 0.6625\n",
      "Epoch 00800 | Train Loss 1.0271 | Train Acc 0.6745 | Val Loss 1.0489 | Val Acc 0.6679\n",
      "Epoch 01000 | Train Loss 1.0054 | Train Acc 0.6805 | Val Loss 1.0296 | Val Acc 0.6744\n",
      "Model saved in './saved_models/tagcn_aminer/checkpoint_epoch_1000.pt'.\n",
      "Epoch 01200 | Train Loss 0.9888 | Train Acc 0.6852 | Val Loss 1.0196 | Val Acc 0.6762\n",
      "Epoch  1355: reducing learning rate of group 0 to 7.5000e-04.\n",
      "Epoch 01400 | Train Loss 0.9761 | Train Acc 0.6887 | Val Loss 1.0087 | Val Acc 0.6798\n",
      "Model saved in './saved_models/tagcn_aminer/checkpoint_epoch_1500.pt'.\n",
      "Epoch 01600 | Train Loss 0.9668 | Train Acc 0.6914 | Val Loss 1.0006 | Val Acc 0.6829\n",
      "Epoch 01800 | Train Loss 0.9595 | Train Acc 0.6942 | Val Loss 0.9943 | Val Acc 0.6845\n",
      "Epoch 02000 | Train Loss 0.9515 | Train Acc 0.6954 | Val Loss 0.9912 | Val Acc 0.6844\n",
      "Model saved in './saved_models/tagcn_aminer/checkpoint_epoch_2000.pt'.\n",
      "Epoch 02200 | Train Loss 0.9453 | Train Acc 0.6977 | Val Loss 0.9871 | Val Acc 0.6862\n",
      "Epoch  2385: reducing learning rate of group 0 to 5.6250e-04.\n",
      "Epoch 02400 | Train Loss 0.9388 | Train Acc 0.6991 | Val Loss 0.9821 | Val Acc 0.6879\n",
      "Epoch  2439: reducing learning rate of group 0 to 4.2188e-04.\n",
      "Model saved in './saved_models/tagcn_aminer/checkpoint_epoch_2500.pt'.\n",
      "Epoch  2585: reducing learning rate of group 0 to 3.1641e-04.\n",
      "Epoch 02600 | Train Loss 0.9352 | Train Acc 0.7006 | Val Loss 0.9804 | Val Acc 0.6894\n",
      "Epoch  2669: reducing learning rate of group 0 to 2.3730e-04.\n",
      "Epoch  2728: reducing learning rate of group 0 to 1.7798e-04.\n",
      "Epoch  2785: reducing learning rate of group 0 to 1.3348e-04.\n",
      "Epoch 02800 | Train Loss 0.9335 | Train Acc 0.7004 | Val Loss 0.9794 | Val Acc 0.6884\n",
      "Epoch  2874: reducing learning rate of group 0 to 1.0011e-04.\n",
      "Training early stopped.\n",
      "Model saved in './saved_models/tagcn_aminer/checkpoint_epoch_2923_early_stopped.pt'.\n"
     ]
    }
   ],
   "source": [
    "train_acc_list, val_acc_list = trainer.train(model=model,\n",
    "                                             n_epoch=10000,\n",
    "                                             save_dir=\"./saved_models/tagcn_aminer\",\n",
    "                                             eval_every=200,\n",
    "                                             save_after=500,\n",
    "                                             dropout=0.1,\n",
    "                                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-coordinator",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "certified-highlight",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:30:59.693306Z",
     "start_time": "2021-05-06T10:30:54.107412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved_models/tagcn_aminer/checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "atlantic-slovenia",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:30:59.958405Z",
     "start_time": "2021-05-06T10:30:59.695569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.7103\n"
     ]
    }
   ],
   "source": [
    "logits, test_acc = trainer.inference(model)\n",
    "print(\"Test ACC: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-milan",
   "metadata": {},
   "source": [
    "## GIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-committee",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "serious-seattle",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:53:57.407003Z",
     "start_time": "2021-05-06T10:53:57.401297Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.utils import fix_seed, get_num_params\n",
    "\n",
    "fix_seed(seed=0)  # fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nutritional-xerox",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:53:57.889174Z",
     "start_time": "2021-05-06T10:53:57.874727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 114325.\n",
      "GIN(\n",
      "  (layers): ModuleList(\n",
      "    (0): GINConv(\n",
      "      (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): GINConv(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): GINConv(\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=18, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.gin import GIN\n",
    "\n",
    "model = GIN(in_features=num_features, out_features=num_classes, hidden_features=[128, 128, 128], activation=F.relu)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-division",
   "metadata": {},
   "source": [
    "### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "extra-string",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:54:11.194922Z",
     "start_time": "2021-05-06T10:53:59.513804Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.model.trainer import Trainer\n",
    "from grb.utils.normalize import GCNAdjNorm\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(dataset=dataset, \n",
    "                  optimizer=adam, \n",
    "                  loss=F.nll_loss,\n",
    "                  adj_norm_func=GCNAdjNorm,\n",
    "                  lr_scheduler=True,\n",
    "                  early_stop=True,\n",
    "                  device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "broadband-upper",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-23T07:47:29.064192Z",
     "start_time": "2021-04-23T07:39:31.587498Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 2.8910 | Train Acc 0.0148 | Val Loss 2.8911 | Val Acc 0.0158\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_0.pt'.\n",
      "Epoch 00010 | Train Loss 2.6846 | Train Acc 0.1462 | Val Loss 2.6867 | Val Acc 0.1432\n",
      "Epoch 00020 | Train Loss 2.5855 | Train Acc 0.1468 | Val Loss 2.5875 | Val Acc 0.1436\n",
      "Epoch 00030 | Train Loss 2.5777 | Train Acc 0.1471 | Val Loss 2.5783 | Val Acc 0.1447\n",
      "Epoch 00040 | Train Loss 2.5521 | Train Acc 0.1646 | Val Loss 2.5547 | Val Acc 0.1617\n",
      "Epoch 00050 | Train Loss 2.5414 | Train Acc 0.2017 | Val Loss 2.5426 | Val Acc 0.1990\n",
      "Epoch 00060 | Train Loss 2.4400 | Train Acc 0.2216 | Val Loss 2.4418 | Val Acc 0.2204\n",
      "Epoch 00070 | Train Loss 2.3860 | Train Acc 0.2524 | Val Loss 2.3885 | Val Acc 0.2509\n",
      "Epoch 00080 | Train Loss 2.2819 | Train Acc 0.2860 | Val Loss 2.2880 | Val Acc 0.2851\n",
      "Epoch 00090 | Train Loss 2.2652 | Train Acc 0.2960 | Val Loss 2.2673 | Val Acc 0.2956\n",
      "Epoch 00100 | Train Loss 2.1392 | Train Acc 0.3406 | Val Loss 2.1439 | Val Acc 0.3379\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_100.pt'.\n",
      "Epoch 00110 | Train Loss 2.2468 | Train Acc 0.3210 | Val Loss 2.2534 | Val Acc 0.3186\n",
      "Epoch 00120 | Train Loss 2.1027 | Train Acc 0.3510 | Val Loss 2.1096 | Val Acc 0.3488\n",
      "Epoch 00130 | Train Loss 1.9760 | Train Acc 0.3937 | Val Loss 1.9861 | Val Acc 0.3894\n",
      "Epoch 00140 | Train Loss 2.0508 | Train Acc 0.3715 | Val Loss 2.0550 | Val Acc 0.3701\n",
      "Epoch 00150 | Train Loss 1.9712 | Train Acc 0.3983 | Val Loss 1.9802 | Val Acc 0.3956\n",
      "Epoch 00160 | Train Loss 1.8830 | Train Acc 0.4211 | Val Loss 1.8959 | Val Acc 0.4195\n",
      "Epoch 00170 | Train Loss 1.8075 | Train Acc 0.4442 | Val Loss 1.8178 | Val Acc 0.4411\n",
      "Epoch 00180 | Train Loss 1.7184 | Train Acc 0.4739 | Val Loss 1.7315 | Val Acc 0.4707\n",
      "Epoch 00190 | Train Loss 1.7161 | Train Acc 0.4780 | Val Loss 1.7304 | Val Acc 0.4751\n",
      "Epoch 00200 | Train Loss 1.6295 | Train Acc 0.5043 | Val Loss 1.6425 | Val Acc 0.5014\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_200.pt'.\n",
      "Epoch 00210 | Train Loss 1.5766 | Train Acc 0.5191 | Val Loss 1.5917 | Val Acc 0.5166\n",
      "Epoch 00220 | Train Loss 1.5299 | Train Acc 0.5369 | Val Loss 1.5407 | Val Acc 0.5360\n",
      "Epoch 00230 | Train Loss 1.4769 | Train Acc 0.5526 | Val Loss 1.4895 | Val Acc 0.5485\n",
      "Epoch 00240 | Train Loss 1.4289 | Train Acc 0.5654 | Val Loss 1.4443 | Val Acc 0.5614\n",
      "Epoch 00250 | Train Loss 1.4378 | Train Acc 0.5658 | Val Loss 1.4522 | Val Acc 0.5614\n",
      "Epoch 00260 | Train Loss 1.3718 | Train Acc 0.5827 | Val Loss 1.3875 | Val Acc 0.5773\n",
      "Epoch 00270 | Train Loss 1.3258 | Train Acc 0.5932 | Val Loss 1.3438 | Val Acc 0.5890\n",
      "Epoch 00280 | Train Loss 1.3767 | Train Acc 0.5759 | Val Loss 1.3917 | Val Acc 0.5705\n",
      "Epoch 00290 | Train Loss 1.3026 | Train Acc 0.6017 | Val Loss 1.3171 | Val Acc 0.5990\n",
      "Epoch 00300 | Train Loss 1.2691 | Train Acc 0.6099 | Val Loss 1.2843 | Val Acc 0.6067\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_300.pt'.\n",
      "Epoch 00310 | Train Loss 1.2528 | Train Acc 0.6136 | Val Loss 1.2668 | Val Acc 0.6099\n",
      "Epoch 00320 | Train Loss 1.2345 | Train Acc 0.6182 | Val Loss 1.2494 | Val Acc 0.6132\n",
      "Epoch 00330 | Train Loss 1.2357 | Train Acc 0.6178 | Val Loss 1.2516 | Val Acc 0.6149\n",
      "Epoch 00340 | Train Loss 1.2105 | Train Acc 0.6258 | Val Loss 1.2275 | Val Acc 0.6218\n",
      "Epoch 00350 | Train Loss 1.2051 | Train Acc 0.6280 | Val Loss 1.2229 | Val Acc 0.6229\n",
      "Epoch 00360 | Train Loss 1.2078 | Train Acc 0.6274 | Val Loss 1.2246 | Val Acc 0.6216\n",
      "Epoch 00370 | Train Loss 1.3497 | Train Acc 0.5936 | Val Loss 1.3610 | Val Acc 0.5902\n",
      "Epoch   375: reducing learning rate of group 0 to 7.5000e-03.\n",
      "Epoch 00380 | Train Loss 1.2600 | Train Acc 0.6177 | Val Loss 1.2780 | Val Acc 0.6123\n",
      "Epoch 00390 | Train Loss 1.2094 | Train Acc 0.6282 | Val Loss 1.2285 | Val Acc 0.6219\n",
      "Epoch   396: reducing learning rate of group 0 to 5.6250e-03.\n",
      "Epoch 00400 | Train Loss 1.1880 | Train Acc 0.6338 | Val Loss 1.2072 | Val Acc 0.6284\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_400.pt'.\n",
      "Epoch 00410 | Train Loss 1.1766 | Train Acc 0.6356 | Val Loss 1.1962 | Val Acc 0.6300\n",
      "Epoch 00420 | Train Loss 1.1696 | Train Acc 0.6376 | Val Loss 1.1900 | Val Acc 0.6321\n",
      "Epoch 00430 | Train Loss 1.1647 | Train Acc 0.6387 | Val Loss 1.1854 | Val Acc 0.6326\n",
      "Epoch 00440 | Train Loss 1.1600 | Train Acc 0.6397 | Val Loss 1.1808 | Val Acc 0.6335\n",
      "Epoch 00450 | Train Loss 1.1560 | Train Acc 0.6409 | Val Loss 1.1768 | Val Acc 0.6352\n",
      "Epoch 00460 | Train Loss 1.1525 | Train Acc 0.6415 | Val Loss 1.1729 | Val Acc 0.6368\n",
      "Epoch 00470 | Train Loss 1.1482 | Train Acc 0.6427 | Val Loss 1.1710 | Val Acc 0.6354\n",
      "Epoch 00480 | Train Loss 1.1455 | Train Acc 0.6436 | Val Loss 1.1688 | Val Acc 0.6372\n",
      "Epoch 00490 | Train Loss 1.1417 | Train Acc 0.6447 | Val Loss 1.1656 | Val Acc 0.6374\n",
      "Epoch 00500 | Train Loss 1.1388 | Train Acc 0.6452 | Val Loss 1.1630 | Val Acc 0.6385\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_500.pt'.\n",
      "Epoch 00510 | Train Loss 1.1352 | Train Acc 0.6457 | Val Loss 1.1598 | Val Acc 0.6392\n",
      "Epoch 00520 | Train Loss 1.1331 | Train Acc 0.6464 | Val Loss 1.1580 | Val Acc 0.6395\n",
      "Epoch 00530 | Train Loss 1.1319 | Train Acc 0.6463 | Val Loss 1.1571 | Val Acc 0.6393\n",
      "Epoch 00540 | Train Loss 1.1266 | Train Acc 0.6480 | Val Loss 1.1532 | Val Acc 0.6411\n",
      "Epoch 00550 | Train Loss 1.1258 | Train Acc 0.6479 | Val Loss 1.1515 | Val Acc 0.6405\n",
      "Epoch 00560 | Train Loss 1.1243 | Train Acc 0.6484 | Val Loss 1.1499 | Val Acc 0.6405\n",
      "Epoch 00570 | Train Loss 1.1203 | Train Acc 0.6493 | Val Loss 1.1472 | Val Acc 0.6423\n",
      "Epoch 00580 | Train Loss 1.1166 | Train Acc 0.6502 | Val Loss 1.1428 | Val Acc 0.6433\n",
      "Epoch 00590 | Train Loss 1.1157 | Train Acc 0.6509 | Val Loss 1.1431 | Val Acc 0.6441\n",
      "Epoch 00600 | Train Loss 1.1139 | Train Acc 0.6511 | Val Loss 1.1397 | Val Acc 0.6442\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_600.pt'.\n",
      "Epoch 00610 | Train Loss 1.1189 | Train Acc 0.6496 | Val Loss 1.1454 | Val Acc 0.6411\n",
      "Epoch 00620 | Train Loss 1.1103 | Train Acc 0.6519 | Val Loss 1.1380 | Val Acc 0.6445\n",
      "Epoch 00630 | Train Loss 1.1056 | Train Acc 0.6530 | Val Loss 1.1342 | Val Acc 0.6457\n",
      "Epoch 00640 | Train Loss 1.1025 | Train Acc 0.6539 | Val Loss 1.1314 | Val Acc 0.6470\n",
      "Epoch 00650 | Train Loss 1.1014 | Train Acc 0.6540 | Val Loss 1.1304 | Val Acc 0.6474\n",
      "Epoch 00660 | Train Loss 1.1029 | Train Acc 0.6533 | Val Loss 1.1309 | Val Acc 0.6457\n",
      "Epoch 00670 | Train Loss 1.1010 | Train Acc 0.6545 | Val Loss 1.1302 | Val Acc 0.6458\n",
      "Epoch 00680 | Train Loss 1.0965 | Train Acc 0.6555 | Val Loss 1.1263 | Val Acc 0.6484\n",
      "Epoch 00690 | Train Loss 1.0918 | Train Acc 0.6566 | Val Loss 1.1211 | Val Acc 0.6491\n",
      "Epoch 00700 | Train Loss 1.0957 | Train Acc 0.6551 | Val Loss 1.1263 | Val Acc 0.6468\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_700.pt'.\n",
      "Epoch 00710 | Train Loss 1.0955 | Train Acc 0.6556 | Val Loss 1.1276 | Val Acc 0.6472\n",
      "Epoch 00720 | Train Loss 1.0870 | Train Acc 0.6578 | Val Loss 1.1184 | Val Acc 0.6493\n",
      "Epoch 00730 | Train Loss 1.0831 | Train Acc 0.6589 | Val Loss 1.1136 | Val Acc 0.6508\n",
      "Epoch 00740 | Train Loss 1.0839 | Train Acc 0.6590 | Val Loss 1.1144 | Val Acc 0.6498\n",
      "Epoch 00750 | Train Loss 1.0789 | Train Acc 0.6603 | Val Loss 1.1106 | Val Acc 0.6508\n",
      "Epoch 00760 | Train Loss 1.0879 | Train Acc 0.6571 | Val Loss 1.1186 | Val Acc 0.6501\n",
      "Epoch 00770 | Train Loss 1.0773 | Train Acc 0.6602 | Val Loss 1.1088 | Val Acc 0.6525\n",
      "Epoch 00780 | Train Loss 1.0739 | Train Acc 0.6615 | Val Loss 1.1054 | Val Acc 0.6534\n",
      "Epoch 00790 | Train Loss 1.0744 | Train Acc 0.6614 | Val Loss 1.1065 | Val Acc 0.6524\n",
      "Epoch 00800 | Train Loss 1.0699 | Train Acc 0.6622 | Val Loss 1.1017 | Val Acc 0.6528\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_800.pt'.\n",
      "Epoch 00810 | Train Loss 1.0704 | Train Acc 0.6620 | Val Loss 1.1049 | Val Acc 0.6544\n",
      "Epoch 00820 | Train Loss 1.0713 | Train Acc 0.6622 | Val Loss 1.1037 | Val Acc 0.6532\n",
      "Epoch 00830 | Train Loss 1.0662 | Train Acc 0.6633 | Val Loss 1.0994 | Val Acc 0.6551\n",
      "Epoch 00840 | Train Loss 1.0616 | Train Acc 0.6645 | Val Loss 1.0932 | Val Acc 0.6559\n",
      "Epoch 00850 | Train Loss 1.0643 | Train Acc 0.6640 | Val Loss 1.0979 | Val Acc 0.6549\n",
      "Epoch 00860 | Train Loss 1.0586 | Train Acc 0.6649 | Val Loss 1.0927 | Val Acc 0.6565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00870 | Train Loss 1.0568 | Train Acc 0.6660 | Val Loss 1.0919 | Val Acc 0.6567\n",
      "Epoch 00880 | Train Loss 1.0617 | Train Acc 0.6638 | Val Loss 1.0959 | Val Acc 0.6558\n",
      "Epoch 00890 | Train Loss 1.0547 | Train Acc 0.6658 | Val Loss 1.0886 | Val Acc 0.6571\n",
      "Epoch 00900 | Train Loss 1.0555 | Train Acc 0.6658 | Val Loss 1.0917 | Val Acc 0.6565\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_900.pt'.\n",
      "Epoch   908: reducing learning rate of group 0 to 4.2188e-03.\n",
      "Epoch 00910 | Train Loss 1.0503 | Train Acc 0.6679 | Val Loss 1.0860 | Val Acc 0.6578\n",
      "Epoch 00920 | Train Loss 1.0484 | Train Acc 0.6680 | Val Loss 1.0839 | Val Acc 0.6581\n",
      "Epoch 00930 | Train Loss 1.0462 | Train Acc 0.6685 | Val Loss 1.0850 | Val Acc 0.6592\n",
      "Epoch 00940 | Train Loss 1.0453 | Train Acc 0.6692 | Val Loss 1.0831 | Val Acc 0.6586\n",
      "Epoch 00950 | Train Loss 1.0439 | Train Acc 0.6690 | Val Loss 1.0819 | Val Acc 0.6594\n",
      "Epoch 00960 | Train Loss 1.0424 | Train Acc 0.6695 | Val Loss 1.0795 | Val Acc 0.6602\n",
      "Epoch 00970 | Train Loss 1.0412 | Train Acc 0.6698 | Val Loss 1.0791 | Val Acc 0.6615\n",
      "Epoch 00980 | Train Loss 1.0399 | Train Acc 0.6703 | Val Loss 1.0791 | Val Acc 0.6601\n",
      "Epoch 00990 | Train Loss 1.0382 | Train Acc 0.6707 | Val Loss 1.0776 | Val Acc 0.6604\n",
      "Epoch 01000 | Train Loss 1.0379 | Train Acc 0.6708 | Val Loss 1.0776 | Val Acc 0.6613\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_1000.pt'.\n",
      "Epoch 01010 | Train Loss 1.0366 | Train Acc 0.6711 | Val Loss 1.0749 | Val Acc 0.6621\n",
      "Epoch 01020 | Train Loss 1.0369 | Train Acc 0.6711 | Val Loss 1.0783 | Val Acc 0.6612\n",
      "Epoch 01030 | Train Loss 1.0360 | Train Acc 0.6713 | Val Loss 1.0758 | Val Acc 0.6607\n",
      "Epoch 01040 | Train Loss 1.0354 | Train Acc 0.6714 | Val Loss 1.0754 | Val Acc 0.6616\n",
      "Epoch 01050 | Train Loss 1.0322 | Train Acc 0.6723 | Val Loss 1.0739 | Val Acc 0.6631\n",
      "Epoch 01060 | Train Loss 1.0318 | Train Acc 0.6723 | Val Loss 1.0736 | Val Acc 0.6632\n",
      "Epoch 01070 | Train Loss 1.0320 | Train Acc 0.6724 | Val Loss 1.0724 | Val Acc 0.6624\n",
      "Epoch 01080 | Train Loss 1.0295 | Train Acc 0.6729 | Val Loss 1.0727 | Val Acc 0.6634\n",
      "Epoch 01090 | Train Loss 1.0291 | Train Acc 0.6732 | Val Loss 1.0736 | Val Acc 0.6632\n",
      "Epoch 01100 | Train Loss 1.0275 | Train Acc 0.6738 | Val Loss 1.0694 | Val Acc 0.6640\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_1100.pt'.\n",
      "Epoch 01110 | Train Loss 1.0285 | Train Acc 0.6731 | Val Loss 1.0704 | Val Acc 0.6634\n",
      "Epoch 01120 | Train Loss 1.0266 | Train Acc 0.6741 | Val Loss 1.0684 | Val Acc 0.6638\n",
      "Epoch 01130 | Train Loss 1.0268 | Train Acc 0.6737 | Val Loss 1.0699 | Val Acc 0.6627\n",
      "Epoch 01140 | Train Loss 1.0235 | Train Acc 0.6747 | Val Loss 1.0669 | Val Acc 0.6654\n",
      "Epoch 01150 | Train Loss 1.0305 | Train Acc 0.6730 | Val Loss 1.0749 | Val Acc 0.6613\n",
      "Epoch  1157: reducing learning rate of group 0 to 3.1641e-03.\n",
      "Epoch 01160 | Train Loss 1.0231 | Train Acc 0.6747 | Val Loss 1.0692 | Val Acc 0.6644\n",
      "Epoch 01170 | Train Loss 1.0210 | Train Acc 0.6752 | Val Loss 1.0665 | Val Acc 0.6659\n",
      "Epoch 01180 | Train Loss 1.0201 | Train Acc 0.6758 | Val Loss 1.0667 | Val Acc 0.6660\n",
      "Epoch 01190 | Train Loss 1.0186 | Train Acc 0.6763 | Val Loss 1.0654 | Val Acc 0.6664\n",
      "Epoch 01200 | Train Loss 1.0180 | Train Acc 0.6761 | Val Loss 1.0645 | Val Acc 0.6661\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_1200.pt'.\n",
      "Epoch 01210 | Train Loss 1.0174 | Train Acc 0.6764 | Val Loss 1.0652 | Val Acc 0.6662\n",
      "Epoch  1218: reducing learning rate of group 0 to 2.3730e-03.\n",
      "Epoch 01220 | Train Loss 1.0166 | Train Acc 0.6765 | Val Loss 1.0650 | Val Acc 0.6656\n",
      "Epoch 01230 | Train Loss 1.0158 | Train Acc 0.6769 | Val Loss 1.0639 | Val Acc 0.6666\n",
      "Epoch 01240 | Train Loss 1.0155 | Train Acc 0.6769 | Val Loss 1.0646 | Val Acc 0.6651\n",
      "Epoch 01250 | Train Loss 1.0147 | Train Acc 0.6772 | Val Loss 1.0635 | Val Acc 0.6660\n",
      "Epoch 01260 | Train Loss 1.0150 | Train Acc 0.6767 | Val Loss 1.0636 | Val Acc 0.6653\n",
      "Epoch 01270 | Train Loss 1.0138 | Train Acc 0.6776 | Val Loss 1.0625 | Val Acc 0.6671\n",
      "Epoch 01280 | Train Loss 1.0137 | Train Acc 0.6774 | Val Loss 1.0628 | Val Acc 0.6660\n",
      "Epoch 01290 | Train Loss 1.0129 | Train Acc 0.6774 | Val Loss 1.0616 | Val Acc 0.6673\n",
      "Epoch 01300 | Train Loss 1.0119 | Train Acc 0.6777 | Val Loss 1.0622 | Val Acc 0.6665\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_1300.pt'.\n",
      "Epoch  1306: reducing learning rate of group 0 to 1.7798e-03.\n",
      "Epoch 01310 | Train Loss 1.0118 | Train Acc 0.6780 | Val Loss 1.0613 | Val Acc 0.6669\n",
      "Epoch 01320 | Train Loss 1.0113 | Train Acc 0.6778 | Val Loss 1.0608 | Val Acc 0.6672\n",
      "Epoch  1327: reducing learning rate of group 0 to 1.3348e-03.\n",
      "Training early stopped.\n",
      "Model saved in './saved_models/gin_aminer/checkpoint_epoch_1326_early_stopped.pt'.\n"
     ]
    }
   ],
   "source": [
    "train_acc_list, val_acc_list = trainer.train(model=model,\n",
    "                                             n_epoch=2000,\n",
    "                                             save_dir=\"./saved_models/gin_aminer\",\n",
    "                                             eval_every=10,\n",
    "                                             save_after=100,\n",
    "                                             dropout=0.1,\n",
    "                                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eb02e5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:54:30.301999Z",
     "start_time": "2021-05-06T10:54:30.252662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved_models/gin_aminer/checkpoint_final.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "590ecd9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:54:30.835510Z",
     "start_time": "2021-05-06T10:54:30.659005Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 23.70 GiB total capacity; 5.87 GiB already allocated; 34.56 MiB free; 5.92 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-575f64fb7126>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test ACC: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/grb/grb/model/trainer.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/grb/grb/model/gin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj, dropout)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/grb/grb/model/gin.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, adj, dropout)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 324.00 MiB (GPU 0; 23.70 GiB total capacity; 5.87 GiB already allocated; 34.56 MiB free; 5.92 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "logits, test_acc = trainer.inference(model)\n",
    "print(\"Test ACC: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2277f0d2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## SGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a83fb0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "740775ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:29:29.446575Z",
     "start_time": "2021-05-06T01:29:29.428243Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 48474.\n",
      "SGCN(\n",
      "  (batchnorm): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (in_conv): Linear(in_features=100, out_features=128, bias=True)\n",
      "  (out_conv): Linear(in_features=128, out_features=18, bias=True)\n",
      "  (layers): ModuleList(\n",
      "    (0): SGConv(\n",
      "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): SGConv(\n",
      "      (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.sgcn import SGCN\n",
    "\n",
    "model = SGCN(in_features=num_features, out_features=num_classes, hidden_features=[128, 128, 128], activation=F.relu)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e26161",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0052394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:30:28.270907Z",
     "start_time": "2021-05-06T01:30:20.235104Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(dataset=dataset, \n",
    "                  optimizer=adam, \n",
    "                  loss=F.nll_loss,\n",
    "                  adj_norm_func=GCNAdjNorm,\n",
    "                  lr_scheduler=True,\n",
    "                  early_stop=True,\n",
    "                  device='cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "451f09e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:48:33.481602Z",
     "start_time": "2021-05-06T01:30:28.272610Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 2.7566 | Train Acc 0.1463 | Val Loss 2.7575 | Val Acc 0.1433\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_0.pt'.\n",
      "Epoch 00010 | Train Loss 2.5920 | Train Acc 0.1553 | Val Loss 2.5936 | Val Acc 0.1525\n",
      "Epoch 00020 | Train Loss 2.4091 | Train Acc 0.2529 | Val Loss 2.4101 | Val Acc 0.2517\n",
      "Epoch 00030 | Train Loss 2.1119 | Train Acc 0.3375 | Val Loss 2.1189 | Val Acc 0.3371\n",
      "Epoch 00040 | Train Loss 1.9016 | Train Acc 0.4171 | Val Loss 1.9082 | Val Acc 0.4155\n",
      "Epoch 00050 | Train Loss 1.7275 | Train Acc 0.4784 | Val Loss 1.7401 | Val Acc 0.4732\n",
      "Epoch 00060 | Train Loss 1.6219 | Train Acc 0.5176 | Val Loss 1.6326 | Val Acc 0.5143\n",
      "Epoch 00070 | Train Loss 1.5498 | Train Acc 0.5431 | Val Loss 1.5614 | Val Acc 0.5399\n",
      "Epoch 00080 | Train Loss 1.4836 | Train Acc 0.5632 | Val Loss 1.4990 | Val Acc 0.5608\n",
      "Epoch 00090 | Train Loss 1.4391 | Train Acc 0.5761 | Val Loss 1.4507 | Val Acc 0.5733\n",
      "Epoch 00100 | Train Loss 1.4104 | Train Acc 0.5856 | Val Loss 1.4205 | Val Acc 0.5822\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_100.pt'.\n",
      "Epoch 00110 | Train Loss 1.3869 | Train Acc 0.5919 | Val Loss 1.4010 | Val Acc 0.5864\n",
      "Epoch 00120 | Train Loss 1.3657 | Train Acc 0.5966 | Val Loss 1.3796 | Val Acc 0.5951\n",
      "Epoch 00130 | Train Loss 1.3469 | Train Acc 0.6011 | Val Loss 1.3563 | Val Acc 0.5990\n",
      "Epoch 00140 | Train Loss 1.3318 | Train Acc 0.6049 | Val Loss 1.3406 | Val Acc 0.6007\n",
      "Epoch 00150 | Train Loss 1.3183 | Train Acc 0.6089 | Val Loss 1.3271 | Val Acc 0.6059\n",
      "Epoch 00160 | Train Loss 1.3046 | Train Acc 0.6120 | Val Loss 1.3166 | Val Acc 0.6083\n",
      "Epoch 00170 | Train Loss 1.2954 | Train Acc 0.6144 | Val Loss 1.3094 | Val Acc 0.6098\n",
      "Epoch 00180 | Train Loss 1.2820 | Train Acc 0.6179 | Val Loss 1.2943 | Val Acc 0.6147\n",
      "Epoch 00190 | Train Loss 1.2736 | Train Acc 0.6199 | Val Loss 1.2887 | Val Acc 0.6172\n",
      "Epoch 00200 | Train Loss 1.2667 | Train Acc 0.6216 | Val Loss 1.2811 | Val Acc 0.6170\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_200.pt'.\n",
      "Epoch 00210 | Train Loss 1.2576 | Train Acc 0.6240 | Val Loss 1.2733 | Val Acc 0.6195\n",
      "Epoch 00220 | Train Loss 1.2521 | Train Acc 0.6263 | Val Loss 1.2628 | Val Acc 0.6237\n",
      "Epoch 00230 | Train Loss 1.2454 | Train Acc 0.6275 | Val Loss 1.2594 | Val Acc 0.6237\n",
      "Epoch 00240 | Train Loss 1.2383 | Train Acc 0.6288 | Val Loss 1.2502 | Val Acc 0.6252\n",
      "Epoch 00250 | Train Loss 1.2337 | Train Acc 0.6305 | Val Loss 1.2423 | Val Acc 0.6271\n",
      "Epoch 00260 | Train Loss 1.2329 | Train Acc 0.6310 | Val Loss 1.2400 | Val Acc 0.6295\n",
      "Epoch 00270 | Train Loss 1.2269 | Train Acc 0.6316 | Val Loss 1.2429 | Val Acc 0.6280\n",
      "Epoch 00280 | Train Loss 1.2231 | Train Acc 0.6329 | Val Loss 1.2383 | Val Acc 0.6313\n",
      "Epoch 00290 | Train Loss 1.2193 | Train Acc 0.6333 | Val Loss 1.2329 | Val Acc 0.6292\n",
      "Epoch 00300 | Train Loss 1.2184 | Train Acc 0.6340 | Val Loss 1.2335 | Val Acc 0.6297\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_300.pt'.\n",
      "Epoch 00310 | Train Loss 1.2423 | Train Acc 0.6301 | Val Loss 1.2569 | Val Acc 0.6263\n",
      "Epoch 00320 | Train Loss 1.2228 | Train Acc 0.6331 | Val Loss 1.2384 | Val Acc 0.6284\n",
      "Epoch 00330 | Train Loss 1.2096 | Train Acc 0.6361 | Val Loss 1.2258 | Val Acc 0.6330\n",
      "Epoch 00340 | Train Loss 1.2057 | Train Acc 0.6366 | Val Loss 1.2215 | Val Acc 0.6325\n",
      "Epoch 00350 | Train Loss 1.2126 | Train Acc 0.6350 | Val Loss 1.2259 | Val Acc 0.6298\n",
      "Epoch 00360 | Train Loss 1.2067 | Train Acc 0.6373 | Val Loss 1.2211 | Val Acc 0.6357\n",
      "Epoch 00370 | Train Loss 1.2014 | Train Acc 0.6374 | Val Loss 1.2142 | Val Acc 0.6351\n",
      "Epoch 00380 | Train Loss 1.1968 | Train Acc 0.6393 | Val Loss 1.2076 | Val Acc 0.6363\n",
      "Epoch 00390 | Train Loss 1.1935 | Train Acc 0.6406 | Val Loss 1.2081 | Val Acc 0.6371\n",
      "Epoch 00400 | Train Loss 1.1920 | Train Acc 0.6407 | Val Loss 1.2057 | Val Acc 0.6358\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_400.pt'.\n",
      "Epoch 00410 | Train Loss 1.1901 | Train Acc 0.6411 | Val Loss 1.2068 | Val Acc 0.6364\n",
      "Epoch 00420 | Train Loss 1.1889 | Train Acc 0.6407 | Val Loss 1.2055 | Val Acc 0.6370\n",
      "Epoch 00430 | Train Loss 1.1877 | Train Acc 0.6413 | Val Loss 1.2020 | Val Acc 0.6374\n",
      "Epoch 00440 | Train Loss 1.1844 | Train Acc 0.6426 | Val Loss 1.1992 | Val Acc 0.6374\n",
      "Epoch 00450 | Train Loss 1.1839 | Train Acc 0.6426 | Val Loss 1.1989 | Val Acc 0.6398\n",
      "Epoch 00460 | Train Loss 1.1840 | Train Acc 0.6422 | Val Loss 1.1988 | Val Acc 0.6385\n",
      "Epoch 00470 | Train Loss 1.1827 | Train Acc 0.6428 | Val Loss 1.1982 | Val Acc 0.6395\n",
      "Epoch 00480 | Train Loss 1.1810 | Train Acc 0.6434 | Val Loss 1.1996 | Val Acc 0.6392\n",
      "Epoch 00490 | Train Loss 1.1814 | Train Acc 0.6435 | Val Loss 1.1974 | Val Acc 0.6399\n",
      "Epoch 00500 | Train Loss 1.1788 | Train Acc 0.6439 | Val Loss 1.1922 | Val Acc 0.6403\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_500.pt'.\n",
      "Epoch 00510 | Train Loss 1.1774 | Train Acc 0.6441 | Val Loss 1.1927 | Val Acc 0.6384\n",
      "Epoch 00520 | Train Loss 1.1759 | Train Acc 0.6446 | Val Loss 1.1933 | Val Acc 0.6402\n",
      "Epoch 00530 | Train Loss 1.1763 | Train Acc 0.6439 | Val Loss 1.1912 | Val Acc 0.6400\n",
      "Epoch 00540 | Train Loss 1.1732 | Train Acc 0.6447 | Val Loss 1.1905 | Val Acc 0.6404\n",
      "Epoch 00550 | Train Loss 1.1752 | Train Acc 0.6448 | Val Loss 1.1904 | Val Acc 0.6430\n",
      "Epoch 00560 | Train Loss 1.1725 | Train Acc 0.6460 | Val Loss 1.1853 | Val Acc 0.6415\n",
      "Epoch 00570 | Train Loss 1.1704 | Train Acc 0.6466 | Val Loss 1.1835 | Val Acc 0.6416\n",
      "Epoch 00580 | Train Loss 1.1721 | Train Acc 0.6453 | Val Loss 1.1869 | Val Acc 0.6394\n",
      "Epoch 00590 | Train Loss 1.1685 | Train Acc 0.6459 | Val Loss 1.1812 | Val Acc 0.6442\n",
      "Epoch 00600 | Train Loss 1.1677 | Train Acc 0.6453 | Val Loss 1.1841 | Val Acc 0.6397\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_600.pt'.\n",
      "Epoch 00610 | Train Loss 1.1656 | Train Acc 0.6460 | Val Loss 1.1830 | Val Acc 0.6421\n",
      "Epoch 00620 | Train Loss 1.1644 | Train Acc 0.6471 | Val Loss 1.1817 | Val Acc 0.6424\n",
      "Epoch 00630 | Train Loss 1.1636 | Train Acc 0.6470 | Val Loss 1.1764 | Val Acc 0.6447\n",
      "Epoch 00640 | Train Loss 1.1639 | Train Acc 0.6475 | Val Loss 1.1760 | Val Acc 0.6425\n",
      "Epoch 00650 | Train Loss 1.1623 | Train Acc 0.6471 | Val Loss 1.1751 | Val Acc 0.6441\n",
      "Epoch 00660 | Train Loss 1.1616 | Train Acc 0.6471 | Val Loss 1.1759 | Val Acc 0.6429\n",
      "Epoch 00670 | Train Loss 1.1612 | Train Acc 0.6487 | Val Loss 1.1761 | Val Acc 0.6450\n",
      "Epoch 00680 | Train Loss 1.1595 | Train Acc 0.6481 | Val Loss 1.1772 | Val Acc 0.6443\n",
      "Epoch 00690 | Train Loss 1.1585 | Train Acc 0.6475 | Val Loss 1.1734 | Val Acc 0.6438\n",
      "Epoch 00700 | Train Loss 1.1591 | Train Acc 0.6476 | Val Loss 1.1732 | Val Acc 0.6426\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_700.pt'.\n",
      "Epoch 00710 | Train Loss 1.1559 | Train Acc 0.6496 | Val Loss 1.1713 | Val Acc 0.6459\n",
      "Epoch 00720 | Train Loss 1.1550 | Train Acc 0.6501 | Val Loss 1.1705 | Val Acc 0.6457\n",
      "Epoch 00730 | Train Loss 1.1546 | Train Acc 0.6493 | Val Loss 1.1713 | Val Acc 0.6467\n",
      "Epoch 00740 | Train Loss 1.1526 | Train Acc 0.6499 | Val Loss 1.1688 | Val Acc 0.6458\n",
      "Epoch 00750 | Train Loss 1.1535 | Train Acc 0.6503 | Val Loss 1.1685 | Val Acc 0.6455\n",
      "Epoch 00760 | Train Loss 1.1528 | Train Acc 0.6496 | Val Loss 1.1698 | Val Acc 0.6438\n",
      "Epoch 00770 | Train Loss 1.1520 | Train Acc 0.6498 | Val Loss 1.1730 | Val Acc 0.6449\n",
      "Epoch 00780 | Train Loss 1.1520 | Train Acc 0.6498 | Val Loss 1.1696 | Val Acc 0.6476\n",
      "Epoch 00790 | Train Loss 1.1517 | Train Acc 0.6512 | Val Loss 1.1724 | Val Acc 0.6461\n",
      "Epoch 00800 | Train Loss 1.1513 | Train Acc 0.6504 | Val Loss 1.1677 | Val Acc 0.6467\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_800.pt'.\n",
      "Epoch 00810 | Train Loss 1.1519 | Train Acc 0.6502 | Val Loss 1.1714 | Val Acc 0.6461\n",
      "Epoch 00820 | Train Loss 1.1500 | Train Acc 0.6508 | Val Loss 1.1670 | Val Acc 0.6476\n",
      "Epoch   824: reducing learning rate of group 0 to 7.5000e-03.\n",
      "Epoch 00830 | Train Loss 1.1472 | Train Acc 0.6515 | Val Loss 1.1654 | Val Acc 0.6468\n",
      "Epoch 00840 | Train Loss 1.1462 | Train Acc 0.6522 | Val Loss 1.1655 | Val Acc 0.6484\n",
      "Epoch 00850 | Train Loss 1.1461 | Train Acc 0.6522 | Val Loss 1.1638 | Val Acc 0.6493\n",
      "Epoch 00860 | Train Loss 1.1446 | Train Acc 0.6524 | Val Loss 1.1635 | Val Acc 0.6480\n",
      "Epoch 00870 | Train Loss 1.1460 | Train Acc 0.6517 | Val Loss 1.1633 | Val Acc 0.6478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00880 | Train Loss 1.1438 | Train Acc 0.6525 | Val Loss 1.1590 | Val Acc 0.6504\n",
      "Epoch 00890 | Train Loss 1.1444 | Train Acc 0.6522 | Val Loss 1.1627 | Val Acc 0.6468\n",
      "Epoch 00900 | Train Loss 1.1433 | Train Acc 0.6529 | Val Loss 1.1601 | Val Acc 0.6463\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_900.pt'.\n",
      "Epoch 00910 | Train Loss 1.1422 | Train Acc 0.6533 | Val Loss 1.1567 | Val Acc 0.6490\n",
      "Epoch 00920 | Train Loss 1.1424 | Train Acc 0.6532 | Val Loss 1.1643 | Val Acc 0.6484\n",
      "Epoch 00930 | Train Loss 1.1409 | Train Acc 0.6529 | Val Loss 1.1595 | Val Acc 0.6489\n",
      "Epoch 00940 | Train Loss 1.1425 | Train Acc 0.6533 | Val Loss 1.1598 | Val Acc 0.6475\n",
      "Epoch 00950 | Train Loss 1.1417 | Train Acc 0.6524 | Val Loss 1.1586 | Val Acc 0.6488\n",
      "Epoch 00960 | Train Loss 1.1417 | Train Acc 0.6534 | Val Loss 1.1595 | Val Acc 0.6489\n",
      "Epoch 00970 | Train Loss 1.1405 | Train Acc 0.6533 | Val Loss 1.1567 | Val Acc 0.6504\n",
      "Epoch 00980 | Train Loss 1.1401 | Train Acc 0.6539 | Val Loss 1.1543 | Val Acc 0.6495\n",
      "Epoch 00990 | Train Loss 1.1371 | Train Acc 0.6537 | Val Loss 1.1539 | Val Acc 0.6504\n",
      "Epoch 01000 | Train Loss 1.1379 | Train Acc 0.6536 | Val Loss 1.1522 | Val Acc 0.6511\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1000.pt'.\n",
      "Epoch 01010 | Train Loss 1.1361 | Train Acc 0.6538 | Val Loss 1.1541 | Val Acc 0.6492\n",
      "Epoch 01020 | Train Loss 1.1363 | Train Acc 0.6544 | Val Loss 1.1550 | Val Acc 0.6485\n",
      "Epoch 01030 | Train Loss 1.1375 | Train Acc 0.6542 | Val Loss 1.1538 | Val Acc 0.6511\n",
      "Epoch  1041: reducing learning rate of group 0 to 5.6250e-03.\n",
      "Epoch 01040 | Train Loss 1.1359 | Train Acc 0.6545 | Val Loss 1.1540 | Val Acc 0.6485\n",
      "Epoch 01050 | Train Loss 1.1338 | Train Acc 0.6544 | Val Loss 1.1485 | Val Acc 0.6500\n",
      "Epoch 01060 | Train Loss 1.1330 | Train Acc 0.6545 | Val Loss 1.1519 | Val Acc 0.6495\n",
      "Epoch 01070 | Train Loss 1.1352 | Train Acc 0.6537 | Val Loss 1.1540 | Val Acc 0.6505\n",
      "Epoch 01080 | Train Loss 1.1333 | Train Acc 0.6545 | Val Loss 1.1531 | Val Acc 0.6490\n",
      "Epoch 01090 | Train Loss 1.1339 | Train Acc 0.6547 | Val Loss 1.1480 | Val Acc 0.6522\n",
      "Epoch 01100 | Train Loss 1.1343 | Train Acc 0.6543 | Val Loss 1.1504 | Val Acc 0.6525\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1100.pt'.\n",
      "Epoch 01110 | Train Loss 1.1322 | Train Acc 0.6561 | Val Loss 1.1503 | Val Acc 0.6501\n",
      "Epoch 01120 | Train Loss 1.1329 | Train Acc 0.6546 | Val Loss 1.1500 | Val Acc 0.6505\n",
      "Epoch 01130 | Train Loss 1.1337 | Train Acc 0.6552 | Val Loss 1.1452 | Val Acc 0.6504\n",
      "Epoch 01140 | Train Loss 1.1325 | Train Acc 0.6552 | Val Loss 1.1497 | Val Acc 0.6485\n",
      "Epoch 01150 | Train Loss 1.1318 | Train Acc 0.6553 | Val Loss 1.1509 | Val Acc 0.6510\n",
      "Epoch 01160 | Train Loss 1.1304 | Train Acc 0.6557 | Val Loss 1.1489 | Val Acc 0.6498\n",
      "Epoch 01170 | Train Loss 1.1289 | Train Acc 0.6563 | Val Loss 1.1473 | Val Acc 0.6521\n",
      "Epoch 01180 | Train Loss 1.1278 | Train Acc 0.6560 | Val Loss 1.1453 | Val Acc 0.6517\n",
      "Epoch 01190 | Train Loss 1.1272 | Train Acc 0.6560 | Val Loss 1.1477 | Val Acc 0.6516\n",
      "Epoch 01200 | Train Loss 1.1260 | Train Acc 0.6573 | Val Loss 1.1479 | Val Acc 0.6522\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1200.pt'.\n",
      "Epoch 01210 | Train Loss 1.1281 | Train Acc 0.6566 | Val Loss 1.1482 | Val Acc 0.6509\n",
      "Epoch 01220 | Train Loss 1.1294 | Train Acc 0.6557 | Val Loss 1.1494 | Val Acc 0.6521\n",
      "Epoch 01230 | Train Loss 1.1296 | Train Acc 0.6552 | Val Loss 1.1438 | Val Acc 0.6519\n",
      "Epoch 01240 | Train Loss 1.1295 | Train Acc 0.6577 | Val Loss 1.1507 | Val Acc 0.6524\n",
      "Epoch 01250 | Train Loss 1.1275 | Train Acc 0.6563 | Val Loss 1.1450 | Val Acc 0.6527\n",
      "Epoch 01260 | Train Loss 1.1267 | Train Acc 0.6562 | Val Loss 1.1436 | Val Acc 0.6513\n",
      "Epoch 01270 | Train Loss 1.1252 | Train Acc 0.6568 | Val Loss 1.1473 | Val Acc 0.6515\n",
      "Epoch  1278: reducing learning rate of group 0 to 4.2188e-03.\n",
      "Epoch 01280 | Train Loss 1.1256 | Train Acc 0.6567 | Val Loss 1.1440 | Val Acc 0.6521\n",
      "Epoch 01290 | Train Loss 1.1253 | Train Acc 0.6570 | Val Loss 1.1433 | Val Acc 0.6518\n",
      "Epoch 01300 | Train Loss 1.1250 | Train Acc 0.6567 | Val Loss 1.1427 | Val Acc 0.6530\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1300.pt'.\n",
      "Epoch 01310 | Train Loss 1.1248 | Train Acc 0.6568 | Val Loss 1.1444 | Val Acc 0.6514\n",
      "Epoch 01320 | Train Loss 1.1249 | Train Acc 0.6567 | Val Loss 1.1406 | Val Acc 0.6526\n",
      "Epoch 01330 | Train Loss 1.1242 | Train Acc 0.6574 | Val Loss 1.1392 | Val Acc 0.6527\n",
      "Epoch 01340 | Train Loss 1.1228 | Train Acc 0.6576 | Val Loss 1.1432 | Val Acc 0.6526\n",
      "Epoch 01350 | Train Loss 1.1233 | Train Acc 0.6576 | Val Loss 1.1404 | Val Acc 0.6534\n",
      "Epoch 01360 | Train Loss 1.1249 | Train Acc 0.6567 | Val Loss 1.1436 | Val Acc 0.6507\n",
      "Epoch 01370 | Train Loss 1.1231 | Train Acc 0.6575 | Val Loss 1.1436 | Val Acc 0.6525\n",
      "Epoch 01380 | Train Loss 1.1232 | Train Acc 0.6574 | Val Loss 1.1407 | Val Acc 0.6526\n",
      "Epoch 01390 | Train Loss 1.1232 | Train Acc 0.6572 | Val Loss 1.1412 | Val Acc 0.6515\n",
      "Epoch 01400 | Train Loss 1.1213 | Train Acc 0.6583 | Val Loss 1.1379 | Val Acc 0.6543\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1400.pt'.\n",
      "Epoch 01410 | Train Loss 1.1222 | Train Acc 0.6582 | Val Loss 1.1414 | Val Acc 0.6551\n",
      "Epoch 01420 | Train Loss 1.1209 | Train Acc 0.6585 | Val Loss 1.1370 | Val Acc 0.6537\n",
      "Epoch 01430 | Train Loss 1.1225 | Train Acc 0.6584 | Val Loss 1.1393 | Val Acc 0.6528\n",
      "Epoch 01440 | Train Loss 1.1219 | Train Acc 0.6579 | Val Loss 1.1416 | Val Acc 0.6533\n",
      "Epoch 01450 | Train Loss 1.1202 | Train Acc 0.6585 | Val Loss 1.1387 | Val Acc 0.6530\n",
      "Epoch 01460 | Train Loss 1.1209 | Train Acc 0.6574 | Val Loss 1.1393 | Val Acc 0.6540\n",
      "Epoch 01470 | Train Loss 1.1202 | Train Acc 0.6587 | Val Loss 1.1388 | Val Acc 0.6523\n",
      "Epoch 01480 | Train Loss 1.1202 | Train Acc 0.6589 | Val Loss 1.1398 | Val Acc 0.6544\n",
      "Epoch 01490 | Train Loss 1.1202 | Train Acc 0.6587 | Val Loss 1.1433 | Val Acc 0.6529\n",
      "Epoch 01500 | Train Loss 1.1191 | Train Acc 0.6587 | Val Loss 1.1378 | Val Acc 0.6536\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1500.pt'.\n",
      "Epoch 01510 | Train Loss 1.1191 | Train Acc 0.6581 | Val Loss 1.1367 | Val Acc 0.6541\n",
      "Epoch  1518: reducing learning rate of group 0 to 3.1641e-03.\n",
      "Epoch 01520 | Train Loss 1.1180 | Train Acc 0.6591 | Val Loss 1.1340 | Val Acc 0.6560\n",
      "Epoch 01530 | Train Loss 1.1175 | Train Acc 0.6589 | Val Loss 1.1378 | Val Acc 0.6548\n",
      "Epoch 01540 | Train Loss 1.1174 | Train Acc 0.6580 | Val Loss 1.1346 | Val Acc 0.6519\n",
      "Epoch 01550 | Train Loss 1.1201 | Train Acc 0.6583 | Val Loss 1.1349 | Val Acc 0.6549\n",
      "Epoch 01560 | Train Loss 1.1172 | Train Acc 0.6591 | Val Loss 1.1349 | Val Acc 0.6534\n",
      "Epoch 01570 | Train Loss 1.1178 | Train Acc 0.6587 | Val Loss 1.1358 | Val Acc 0.6550\n",
      "Epoch 01580 | Train Loss 1.1168 | Train Acc 0.6591 | Val Loss 1.1348 | Val Acc 0.6536\n",
      "Epoch 01590 | Train Loss 1.1163 | Train Acc 0.6585 | Val Loss 1.1340 | Val Acc 0.6543\n",
      "Epoch 01600 | Train Loss 1.1169 | Train Acc 0.6594 | Val Loss 1.1329 | Val Acc 0.6548\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1600.pt'.\n",
      "Epoch  1604: reducing learning rate of group 0 to 2.3730e-03.\n",
      "Epoch 01610 | Train Loss 1.1157 | Train Acc 0.6593 | Val Loss 1.1345 | Val Acc 0.6539\n",
      "Epoch 01620 | Train Loss 1.1160 | Train Acc 0.6592 | Val Loss 1.1388 | Val Acc 0.6534\n",
      "Epoch 01630 | Train Loss 1.1160 | Train Acc 0.6591 | Val Loss 1.1333 | Val Acc 0.6538\n",
      "Epoch 01640 | Train Loss 1.1143 | Train Acc 0.6593 | Val Loss 1.1355 | Val Acc 0.6544\n",
      "Epoch 01650 | Train Loss 1.1139 | Train Acc 0.6599 | Val Loss 1.1339 | Val Acc 0.6551\n",
      "Epoch 01660 | Train Loss 1.1149 | Train Acc 0.6594 | Val Loss 1.1311 | Val Acc 0.6565\n",
      "Epoch  1671: reducing learning rate of group 0 to 1.7798e-03.\n",
      "Epoch 01670 | Train Loss 1.1140 | Train Acc 0.6600 | Val Loss 1.1326 | Val Acc 0.6543\n",
      "Epoch 01680 | Train Loss 1.1155 | Train Acc 0.6592 | Val Loss 1.1309 | Val Acc 0.6555\n",
      "Epoch 01690 | Train Loss 1.1140 | Train Acc 0.6598 | Val Loss 1.1301 | Val Acc 0.6538\n",
      "Epoch 01700 | Train Loss 1.1131 | Train Acc 0.6596 | Val Loss 1.1341 | Val Acc 0.6539\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1700.pt'.\n",
      "Epoch 01710 | Train Loss 1.1144 | Train Acc 0.6602 | Val Loss 1.1341 | Val Acc 0.6547\n",
      "Epoch 01720 | Train Loss 1.1131 | Train Acc 0.6603 | Val Loss 1.1299 | Val Acc 0.6552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01730 | Train Loss 1.1128 | Train Acc 0.6596 | Val Loss 1.1296 | Val Acc 0.6546\n",
      "Epoch 01740 | Train Loss 1.1123 | Train Acc 0.6604 | Val Loss 1.1272 | Val Acc 0.6564\n",
      "Epoch 01750 | Train Loss 1.1112 | Train Acc 0.6604 | Val Loss 1.1274 | Val Acc 0.6552\n",
      "Epoch 01760 | Train Loss 1.1116 | Train Acc 0.6606 | Val Loss 1.1311 | Val Acc 0.6559\n",
      "Epoch 01770 | Train Loss 1.1113 | Train Acc 0.6598 | Val Loss 1.1326 | Val Acc 0.6548\n",
      "Epoch  1779: reducing learning rate of group 0 to 1.3348e-03.\n",
      "Epoch 01780 | Train Loss 1.1120 | Train Acc 0.6602 | Val Loss 1.1277 | Val Acc 0.6556\n",
      "Epoch 01790 | Train Loss 1.1111 | Train Acc 0.6608 | Val Loss 1.1281 | Val Acc 0.6566\n",
      "Epoch 01800 | Train Loss 1.1120 | Train Acc 0.6601 | Val Loss 1.1366 | Val Acc 0.6549\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1800.pt'.\n",
      "Epoch 01810 | Train Loss 1.1125 | Train Acc 0.6602 | Val Loss 1.1287 | Val Acc 0.6545\n",
      "Epoch 01820 | Train Loss 1.1121 | Train Acc 0.6607 | Val Loss 1.1315 | Val Acc 0.6560\n",
      "Epoch 01830 | Train Loss 1.1115 | Train Acc 0.6603 | Val Loss 1.1300 | Val Acc 0.6546\n",
      "Epoch 01840 | Train Loss 1.1101 | Train Acc 0.6609 | Val Loss 1.1286 | Val Acc 0.6557\n",
      "Epoch 01850 | Train Loss 1.1110 | Train Acc 0.6606 | Val Loss 1.1284 | Val Acc 0.6546\n",
      "Epoch 01860 | Train Loss 1.1100 | Train Acc 0.6605 | Val Loss 1.1297 | Val Acc 0.6565\n",
      "Epoch 01870 | Train Loss 1.1083 | Train Acc 0.6604 | Val Loss 1.1271 | Val Acc 0.6563\n",
      "Epoch  1876: reducing learning rate of group 0 to 1.0011e-03.\n",
      "Epoch 01880 | Train Loss 1.1099 | Train Acc 0.6610 | Val Loss 1.1284 | Val Acc 0.6566\n",
      "Epoch 01890 | Train Loss 1.1086 | Train Acc 0.6609 | Val Loss 1.1303 | Val Acc 0.6556\n",
      "Epoch 01900 | Train Loss 1.1103 | Train Acc 0.6606 | Val Loss 1.1267 | Val Acc 0.6564\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1900.pt'.\n",
      "Epoch 01910 | Train Loss 1.1100 | Train Acc 0.6604 | Val Loss 1.1276 | Val Acc 0.6565\n",
      "Epoch 01920 | Train Loss 1.1081 | Train Acc 0.6609 | Val Loss 1.1279 | Val Acc 0.6561\n",
      "Training early stopped.\n",
      "Model saved in './saved_models/sgcn_aminer/checkpoint_epoch_1925_early_stopped.pt'.\n"
     ]
    }
   ],
   "source": [
    "trainer.train(model=model,\n",
    "              n_epoch=2000,\n",
    "              save_dir=\"./saved_models/sgcn_aminer\",\n",
    "              eval_every=10,\n",
    "              save_after=100,\n",
    "              dropout=0.5,\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ece1bd3",
   "metadata": {},
   "source": [
    "## GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfdc551",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e4061f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:43:42.569460Z",
     "start_time": "2021-05-06T10:43:42.555042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 156184.\n",
      "GraphSAGE(\n",
      "  (layers): ModuleList(\n",
      "    (0): SAGEConv(\n",
      "      (pool_fc): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (fc1): Linear(in_features=100, out_features=128, bias=True)\n",
      "      (fc2): Linear(in_features=100, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): SAGEConv(\n",
      "      (pool_fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): SAGEConv(\n",
      "      (pool_fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (3): SAGEConv(\n",
      "      (pool_fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (fc1): Linear(in_features=128, out_features=18, bias=True)\n",
      "      (fc2): Linear(in_features=128, out_features=18, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.graphsage import GraphSAGE\n",
    "\n",
    "model = GraphSAGE(in_features=num_features, out_features=num_classes, \n",
    "                  hidden_features=[128, 128, 128], activation=F.relu)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc39e5",
   "metadata": {},
   "source": [
    "### Prepare trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04dfb25e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:44:17.595960Z",
     "start_time": "2021-05-06T10:44:05.909251Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.model.trainer import Trainer\n",
    "from grb.utils.normalize import SAGEAdjNorm\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(dataset=dataset, \n",
    "                  optimizer=adam, \n",
    "                  loss=F.nll_loss,\n",
    "                  adj_norm_func=SAGEAdjNorm,\n",
    "                  lr_scheduler=True,\n",
    "                  early_stop=True,\n",
    "                  device='cuda:2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b390601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T02:05:29.190561Z",
     "start_time": "2021-05-06T01:52:52.654872Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 2.8781 | Train Acc 0.0612 | Val Loss 2.8774 | Val Acc 0.0624\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_0.pt'.\n",
      "Epoch 00010 | Train Loss 2.5855 | Train Acc 0.1463 | Val Loss 2.5870 | Val Acc 0.1433\n",
      "Epoch 00020 | Train Loss 2.5783 | Train Acc 0.1463 | Val Loss 2.5792 | Val Acc 0.1433\n",
      "Epoch 00030 | Train Loss 2.5764 | Train Acc 0.1463 | Val Loss 2.5775 | Val Acc 0.1433\n",
      "Epoch 00040 | Train Loss 2.5755 | Train Acc 0.1463 | Val Loss 2.5769 | Val Acc 0.1433\n",
      "Epoch 00050 | Train Loss 2.5750 | Train Acc 0.1463 | Val Loss 2.5765 | Val Acc 0.1433\n",
      "Epoch 00060 | Train Loss 2.5743 | Train Acc 0.1461 | Val Loss 2.5755 | Val Acc 0.1432\n",
      "Epoch 00070 | Train Loss 2.5732 | Train Acc 0.1457 | Val Loss 2.5747 | Val Acc 0.1434\n",
      "Epoch 00080 | Train Loss 2.5725 | Train Acc 0.1461 | Val Loss 2.5737 | Val Acc 0.1422\n",
      "Epoch 00090 | Train Loss 2.5711 | Train Acc 0.1470 | Val Loss 2.5727 | Val Acc 0.1443\n",
      "Epoch 00100 | Train Loss 2.5703 | Train Acc 0.1476 | Val Loss 2.5720 | Val Acc 0.1434\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_100.pt'.\n",
      "Epoch 00110 | Train Loss 2.5682 | Train Acc 0.1482 | Val Loss 2.5700 | Val Acc 0.1454\n",
      "Epoch 00120 | Train Loss 2.5628 | Train Acc 0.1545 | Val Loss 2.5643 | Val Acc 0.1519\n",
      "Epoch 00130 | Train Loss 2.5118 | Train Acc 0.2007 | Val Loss 2.5126 | Val Acc 0.1989\n",
      "Epoch 00140 | Train Loss 2.3841 | Train Acc 0.2370 | Val Loss 2.3853 | Val Acc 0.2335\n",
      "Epoch 00150 | Train Loss 2.3340 | Train Acc 0.2500 | Val Loss 2.3354 | Val Acc 0.2481\n",
      "Epoch 00160 | Train Loss 2.2767 | Train Acc 0.2583 | Val Loss 2.2780 | Val Acc 0.2560\n",
      "Epoch 00170 | Train Loss 2.2285 | Train Acc 0.2676 | Val Loss 2.2278 | Val Acc 0.2670\n",
      "Epoch 00180 | Train Loss 2.1859 | Train Acc 0.2829 | Val Loss 2.1850 | Val Acc 0.2826\n",
      "Epoch 00190 | Train Loss 2.1555 | Train Acc 0.2976 | Val Loss 2.1553 | Val Acc 0.2942\n",
      "Epoch 00200 | Train Loss 2.1417 | Train Acc 0.3024 | Val Loss 2.1419 | Val Acc 0.3015\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_200.pt'.\n",
      "Epoch 00210 | Train Loss 2.0973 | Train Acc 0.3108 | Val Loss 2.0979 | Val Acc 0.3087\n",
      "Epoch 00220 | Train Loss 2.0359 | Train Acc 0.3515 | Val Loss 2.0359 | Val Acc 0.3485\n",
      "Epoch 00230 | Train Loss 2.0148 | Train Acc 0.3585 | Val Loss 2.0191 | Val Acc 0.3547\n",
      "Epoch 00240 | Train Loss 1.9661 | Train Acc 0.3740 | Val Loss 1.9698 | Val Acc 0.3712\n",
      "Epoch 00250 | Train Loss 1.9200 | Train Acc 0.3824 | Val Loss 1.9247 | Val Acc 0.3790\n",
      "Epoch 00260 | Train Loss 1.8690 | Train Acc 0.3988 | Val Loss 1.8759 | Val Acc 0.3936\n",
      "Epoch 00270 | Train Loss 1.8810 | Train Acc 0.3993 | Val Loss 1.8871 | Val Acc 0.3963\n",
      "Epoch 00280 | Train Loss 1.7986 | Train Acc 0.4302 | Val Loss 1.8058 | Val Acc 0.4258\n",
      "Epoch 00290 | Train Loss 1.7917 | Train Acc 0.4415 | Val Loss 1.7979 | Val Acc 0.4379\n",
      "Epoch 00300 | Train Loss 1.7094 | Train Acc 0.4757 | Val Loss 1.7180 | Val Acc 0.4714\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_300.pt'.\n",
      "Epoch 00310 | Train Loss 1.6684 | Train Acc 0.4974 | Val Loss 1.6792 | Val Acc 0.4933\n",
      "Epoch 00320 | Train Loss 1.8195 | Train Acc 0.4457 | Val Loss 1.8275 | Val Acc 0.4435\n",
      "Epoch 00330 | Train Loss 1.6485 | Train Acc 0.5118 | Val Loss 1.6600 | Val Acc 0.5078\n",
      "Epoch 00340 | Train Loss 1.5949 | Train Acc 0.5227 | Val Loss 1.6050 | Val Acc 0.5201\n",
      "Epoch 00350 | Train Loss 1.5573 | Train Acc 0.5352 | Val Loss 1.5694 | Val Acc 0.5310\n",
      "Epoch 00360 | Train Loss 1.5228 | Train Acc 0.5434 | Val Loss 1.5320 | Val Acc 0.5428\n",
      "Epoch 00370 | Train Loss 1.5257 | Train Acc 0.5420 | Val Loss 1.5388 | Val Acc 0.5380\n",
      "Epoch 00380 | Train Loss 1.4801 | Train Acc 0.5539 | Val Loss 1.4908 | Val Acc 0.5517\n",
      "Epoch 00390 | Train Loss 1.4548 | Train Acc 0.5619 | Val Loss 1.4680 | Val Acc 0.5597\n",
      "Epoch 00400 | Train Loss 1.4441 | Train Acc 0.5636 | Val Loss 1.4557 | Val Acc 0.5597\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_400.pt'.\n",
      "Epoch 00410 | Train Loss 1.4328 | Train Acc 0.5678 | Val Loss 1.4437 | Val Acc 0.5654\n",
      "Epoch 00420 | Train Loss 1.4189 | Train Acc 0.5700 | Val Loss 1.4319 | Val Acc 0.5660\n",
      "Epoch 00430 | Train Loss 1.3886 | Train Acc 0.5807 | Val Loss 1.4003 | Val Acc 0.5779\n",
      "Epoch 00440 | Train Loss 1.3746 | Train Acc 0.5832 | Val Loss 1.3909 | Val Acc 0.5808\n",
      "Epoch 00450 | Train Loss 1.3634 | Train Acc 0.5865 | Val Loss 1.3768 | Val Acc 0.5833\n",
      "Epoch 00460 | Train Loss 1.3485 | Train Acc 0.5910 | Val Loss 1.3635 | Val Acc 0.5888\n",
      "Epoch 00470 | Train Loss 1.3462 | Train Acc 0.5906 | Val Loss 1.3588 | Val Acc 0.5887\n",
      "Epoch 00480 | Train Loss 1.3349 | Train Acc 0.5946 | Val Loss 1.3482 | Val Acc 0.5916\n",
      "Epoch 00490 | Train Loss 1.3231 | Train Acc 0.5984 | Val Loss 1.3370 | Val Acc 0.5940\n",
      "Epoch 00500 | Train Loss 1.3075 | Train Acc 0.6029 | Val Loss 1.3250 | Val Acc 0.5974\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_500.pt'.\n",
      "Epoch 00510 | Train Loss 1.3056 | Train Acc 0.6033 | Val Loss 1.3170 | Val Acc 0.6018\n",
      "Epoch 00520 | Train Loss 1.2920 | Train Acc 0.6070 | Val Loss 1.3053 | Val Acc 0.6038\n",
      "Epoch 00530 | Train Loss 1.2815 | Train Acc 0.6107 | Val Loss 1.2971 | Val Acc 0.6074\n",
      "Epoch 00540 | Train Loss 1.2847 | Train Acc 0.6088 | Val Loss 1.3008 | Val Acc 0.6043\n",
      "Epoch 00550 | Train Loss 1.2832 | Train Acc 0.6086 | Val Loss 1.2984 | Val Acc 0.6039\n",
      "Epoch 00560 | Train Loss 1.2671 | Train Acc 0.6144 | Val Loss 1.2828 | Val Acc 0.6091\n",
      "Epoch 00570 | Train Loss 1.2624 | Train Acc 0.6154 | Val Loss 1.2761 | Val Acc 0.6115\n",
      "Epoch 00580 | Train Loss 1.2558 | Train Acc 0.6173 | Val Loss 1.2714 | Val Acc 0.6150\n",
      "Epoch 00590 | Train Loss 1.2494 | Train Acc 0.6190 | Val Loss 1.2613 | Val Acc 0.6160\n",
      "Epoch 00600 | Train Loss 1.2783 | Train Acc 0.6096 | Val Loss 1.2930 | Val Acc 0.6044\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_600.pt'.\n",
      "Epoch 00610 | Train Loss 1.2689 | Train Acc 0.6141 | Val Loss 1.2833 | Val Acc 0.6106\n",
      "Epoch 00620 | Train Loss 1.2472 | Train Acc 0.6200 | Val Loss 1.2614 | Val Acc 0.6166\n",
      "Epoch 00630 | Train Loss 1.2369 | Train Acc 0.6231 | Val Loss 1.2499 | Val Acc 0.6185\n",
      "Epoch 00640 | Train Loss 1.2318 | Train Acc 0.6247 | Val Loss 1.2487 | Val Acc 0.6202\n",
      "Epoch 00650 | Train Loss 1.2308 | Train Acc 0.6240 | Val Loss 1.2474 | Val Acc 0.6196\n",
      "Epoch 00660 | Train Loss 1.2228 | Train Acc 0.6271 | Val Loss 1.2399 | Val Acc 0.6200\n",
      "Epoch 00670 | Train Loss 1.2194 | Train Acc 0.6279 | Val Loss 1.2360 | Val Acc 0.6236\n",
      "Epoch 00680 | Train Loss 1.2166 | Train Acc 0.6285 | Val Loss 1.2308 | Val Acc 0.6247\n",
      "Epoch 00690 | Train Loss 1.2155 | Train Acc 0.6295 | Val Loss 1.2308 | Val Acc 0.6261\n",
      "Epoch 00700 | Train Loss 1.2198 | Train Acc 0.6290 | Val Loss 1.2320 | Val Acc 0.6261\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_700.pt'.\n",
      "Epoch 00710 | Train Loss 1.2109 | Train Acc 0.6312 | Val Loss 1.2287 | Val Acc 0.6260\n",
      "Epoch 00720 | Train Loss 1.2074 | Train Acc 0.6321 | Val Loss 1.2260 | Val Acc 0.6246\n",
      "Epoch 00730 | Train Loss 1.2032 | Train Acc 0.6331 | Val Loss 1.2222 | Val Acc 0.6266\n",
      "Epoch 00740 | Train Loss 1.2002 | Train Acc 0.6346 | Val Loss 1.2162 | Val Acc 0.6290\n",
      "Epoch 00750 | Train Loss 1.2002 | Train Acc 0.6338 | Val Loss 1.2200 | Val Acc 0.6271\n",
      "Epoch 00760 | Train Loss 1.2009 | Train Acc 0.6331 | Val Loss 1.2187 | Val Acc 0.6267\n",
      "Epoch 00770 | Train Loss 1.1922 | Train Acc 0.6365 | Val Loss 1.2095 | Val Acc 0.6303\n",
      "Epoch 00780 | Train Loss 1.1888 | Train Acc 0.6376 | Val Loss 1.2045 | Val Acc 0.6306\n",
      "Epoch 00790 | Train Loss 1.1949 | Train Acc 0.6352 | Val Loss 1.2079 | Val Acc 0.6325\n",
      "Epoch 00800 | Train Loss 1.1883 | Train Acc 0.6374 | Val Loss 1.2024 | Val Acc 0.6343\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_800.pt'.\n",
      "Epoch 00810 | Train Loss 1.1928 | Train Acc 0.6362 | Val Loss 1.2119 | Val Acc 0.6301\n",
      "Epoch 00820 | Train Loss 1.1822 | Train Acc 0.6393 | Val Loss 1.1991 | Val Acc 0.6345\n",
      "Epoch 00830 | Train Loss 1.1763 | Train Acc 0.6403 | Val Loss 1.1951 | Val Acc 0.6336\n",
      "Epoch 00840 | Train Loss 1.1813 | Train Acc 0.6403 | Val Loss 1.1985 | Val Acc 0.6344\n",
      "Epoch 00850 | Train Loss 1.1798 | Train Acc 0.6401 | Val Loss 1.1964 | Val Acc 0.6350\n",
      "Epoch 00860 | Train Loss 1.1804 | Train Acc 0.6397 | Val Loss 1.1961 | Val Acc 0.6346\n",
      "Epoch 00870 | Train Loss 1.1715 | Train Acc 0.6425 | Val Loss 1.1881 | Val Acc 0.6370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00880 | Train Loss 1.1855 | Train Acc 0.6379 | Val Loss 1.2048 | Val Acc 0.6323\n",
      "Epoch 00890 | Train Loss 1.1925 | Train Acc 0.6368 | Val Loss 1.2092 | Val Acc 0.6326\n",
      "Epoch 00900 | Train Loss 1.1793 | Train Acc 0.6389 | Val Loss 1.1944 | Val Acc 0.6344\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_900.pt'.\n",
      "Epoch 00910 | Train Loss 1.1676 | Train Acc 0.6431 | Val Loss 1.1843 | Val Acc 0.6380\n",
      "Epoch 00920 | Train Loss 1.1664 | Train Acc 0.6442 | Val Loss 1.1877 | Val Acc 0.6377\n",
      "Epoch 00930 | Train Loss 1.1629 | Train Acc 0.6446 | Val Loss 1.1773 | Val Acc 0.6394\n",
      "Epoch 00940 | Train Loss 1.1649 | Train Acc 0.6436 | Val Loss 1.1780 | Val Acc 0.6389\n",
      "Epoch 00950 | Train Loss 1.1644 | Train Acc 0.6437 | Val Loss 1.1808 | Val Acc 0.6383\n",
      "Epoch 00960 | Train Loss 1.1571 | Train Acc 0.6462 | Val Loss 1.1750 | Val Acc 0.6402\n",
      "Epoch 00970 | Train Loss 1.1560 | Train Acc 0.6467 | Val Loss 1.1735 | Val Acc 0.6417\n",
      "Epoch 00980 | Train Loss 1.1561 | Train Acc 0.6464 | Val Loss 1.1731 | Val Acc 0.6419\n",
      "Epoch 00990 | Train Loss 1.1517 | Train Acc 0.6477 | Val Loss 1.1721 | Val Acc 0.6422\n",
      "Epoch 01000 | Train Loss 1.1545 | Train Acc 0.6466 | Val Loss 1.1723 | Val Acc 0.6395\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1000.pt'.\n",
      "Epoch 01010 | Train Loss 1.1546 | Train Acc 0.6467 | Val Loss 1.1718 | Val Acc 0.6415\n",
      "Epoch 01020 | Train Loss 1.1572 | Train Acc 0.6455 | Val Loss 1.1731 | Val Acc 0.6417\n",
      "Epoch 01030 | Train Loss 1.1516 | Train Acc 0.6470 | Val Loss 1.1706 | Val Acc 0.6409\n",
      "Epoch 01040 | Train Loss 1.1457 | Train Acc 0.6489 | Val Loss 1.1611 | Val Acc 0.6431\n",
      "Epoch 01050 | Train Loss 1.1483 | Train Acc 0.6485 | Val Loss 1.1679 | Val Acc 0.6431\n",
      "Epoch 01060 | Train Loss 1.1469 | Train Acc 0.6488 | Val Loss 1.1674 | Val Acc 0.6421\n",
      "Epoch 01070 | Train Loss 1.1485 | Train Acc 0.6483 | Val Loss 1.1675 | Val Acc 0.6415\n",
      "Epoch 01080 | Train Loss 1.1644 | Train Acc 0.6442 | Val Loss 1.1823 | Val Acc 0.6393\n",
      "Epoch 01090 | Train Loss 1.1563 | Train Acc 0.6459 | Val Loss 1.1734 | Val Acc 0.6399\n",
      "Epoch 01100 | Train Loss 1.1461 | Train Acc 0.6495 | Val Loss 1.1636 | Val Acc 0.6434\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1100.pt'.\n",
      "Epoch 01110 | Train Loss 1.1428 | Train Acc 0.6497 | Val Loss 1.1589 | Val Acc 0.6456\n",
      "Epoch 01120 | Train Loss 1.1385 | Train Acc 0.6504 | Val Loss 1.1571 | Val Acc 0.6451\n",
      "Epoch 01130 | Train Loss 1.1382 | Train Acc 0.6509 | Val Loss 1.1561 | Val Acc 0.6453\n",
      "Epoch 01140 | Train Loss 1.1499 | Train Acc 0.6475 | Val Loss 1.1667 | Val Acc 0.6449\n",
      "Epoch 01150 | Train Loss 1.1372 | Train Acc 0.6521 | Val Loss 1.1539 | Val Acc 0.6454\n",
      "Epoch 01160 | Train Loss 1.1368 | Train Acc 0.6506 | Val Loss 1.1575 | Val Acc 0.6445\n",
      "Epoch 01170 | Train Loss 1.1408 | Train Acc 0.6496 | Val Loss 1.1618 | Val Acc 0.6436\n",
      "Epoch 01180 | Train Loss 1.1362 | Train Acc 0.6518 | Val Loss 1.1550 | Val Acc 0.6451\n",
      "Epoch 01190 | Train Loss 1.1343 | Train Acc 0.6523 | Val Loss 1.1505 | Val Acc 0.6466\n",
      "Epoch 01200 | Train Loss 1.1336 | Train Acc 0.6525 | Val Loss 1.1519 | Val Acc 0.6471\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1200.pt'.\n",
      "Epoch 01210 | Train Loss 1.1346 | Train Acc 0.6520 | Val Loss 1.1537 | Val Acc 0.6445\n",
      "Epoch 01220 | Train Loss 1.1321 | Train Acc 0.6519 | Val Loss 1.1479 | Val Acc 0.6476\n",
      "Epoch 01230 | Train Loss 1.1288 | Train Acc 0.6532 | Val Loss 1.1460 | Val Acc 0.6471\n",
      "Epoch 01240 | Train Loss 1.1988 | Train Acc 0.6322 | Val Loss 1.2125 | Val Acc 0.6286\n",
      "Epoch 01250 | Train Loss 1.1459 | Train Acc 0.6487 | Val Loss 1.1660 | Val Acc 0.6433\n",
      "Epoch 01260 | Train Loss 1.1420 | Train Acc 0.6493 | Val Loss 1.1596 | Val Acc 0.6424\n",
      "Epoch  1264: reducing learning rate of group 0 to 7.5000e-03.\n",
      "Epoch 01270 | Train Loss 1.1308 | Train Acc 0.6533 | Val Loss 1.1483 | Val Acc 0.6476\n",
      "Epoch 01280 | Train Loss 1.1276 | Train Acc 0.6540 | Val Loss 1.1447 | Val Acc 0.6493\n",
      "Epoch 01290 | Train Loss 1.1251 | Train Acc 0.6551 | Val Loss 1.1389 | Val Acc 0.6515\n",
      "Epoch 01300 | Train Loss 1.1230 | Train Acc 0.6547 | Val Loss 1.1437 | Val Acc 0.6481\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1300.pt'.\n",
      "Epoch 01310 | Train Loss 1.1226 | Train Acc 0.6547 | Val Loss 1.1415 | Val Acc 0.6498\n",
      "Epoch 01320 | Train Loss 1.1220 | Train Acc 0.6551 | Val Loss 1.1377 | Val Acc 0.6511\n",
      "Epoch 01330 | Train Loss 1.1198 | Train Acc 0.6559 | Val Loss 1.1404 | Val Acc 0.6491\n",
      "Epoch 01340 | Train Loss 1.1216 | Train Acc 0.6555 | Val Loss 1.1445 | Val Acc 0.6475\n",
      "Epoch 01350 | Train Loss 1.1275 | Train Acc 0.6535 | Val Loss 1.1466 | Val Acc 0.6481\n",
      "Epoch 01360 | Train Loss 1.1240 | Train Acc 0.6545 | Val Loss 1.1440 | Val Acc 0.6482\n",
      "Epoch  1369: reducing learning rate of group 0 to 5.6250e-03.\n",
      "Epoch 01370 | Train Loss 1.1217 | Train Acc 0.6554 | Val Loss 1.1413 | Val Acc 0.6497\n",
      "Epoch 01380 | Train Loss 1.1208 | Train Acc 0.6554 | Val Loss 1.1364 | Val Acc 0.6503\n",
      "Epoch 01390 | Train Loss 1.1188 | Train Acc 0.6561 | Val Loss 1.1387 | Val Acc 0.6509\n",
      "Epoch 01400 | Train Loss 1.1199 | Train Acc 0.6559 | Val Loss 1.1363 | Val Acc 0.6509\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1400.pt'.\n",
      "Epoch 01410 | Train Loss 1.1165 | Train Acc 0.6573 | Val Loss 1.1387 | Val Acc 0.6509\n",
      "Epoch 01420 | Train Loss 1.1163 | Train Acc 0.6570 | Val Loss 1.1366 | Val Acc 0.6524\n",
      "Epoch 01430 | Train Loss 1.1168 | Train Acc 0.6567 | Val Loss 1.1357 | Val Acc 0.6525\n",
      "Epoch 01440 | Train Loss 1.1150 | Train Acc 0.6563 | Val Loss 1.1367 | Val Acc 0.6505\n",
      "Epoch 01450 | Train Loss 1.1156 | Train Acc 0.6571 | Val Loss 1.1339 | Val Acc 0.6516\n",
      "Epoch 01460 | Train Loss 1.1148 | Train Acc 0.6571 | Val Loss 1.1337 | Val Acc 0.6518\n",
      "Epoch 01470 | Train Loss 1.1149 | Train Acc 0.6569 | Val Loss 1.1325 | Val Acc 0.6514\n",
      "Epoch 01480 | Train Loss 1.1115 | Train Acc 0.6582 | Val Loss 1.1318 | Val Acc 0.6522\n",
      "Epoch 01490 | Train Loss 1.1130 | Train Acc 0.6582 | Val Loss 1.1337 | Val Acc 0.6523\n",
      "Epoch 01500 | Train Loss 1.1130 | Train Acc 0.6580 | Val Loss 1.1336 | Val Acc 0.6522\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1500.pt'.\n",
      "Epoch 01510 | Train Loss 1.1135 | Train Acc 0.6577 | Val Loss 1.1345 | Val Acc 0.6510\n",
      "Epoch 01520 | Train Loss 1.1121 | Train Acc 0.6579 | Val Loss 1.1320 | Val Acc 0.6515\n",
      "Epoch 01530 | Train Loss 1.1113 | Train Acc 0.6580 | Val Loss 1.1300 | Val Acc 0.6532\n",
      "Epoch 01540 | Train Loss 1.1095 | Train Acc 0.6589 | Val Loss 1.1303 | Val Acc 0.6523\n",
      "Epoch 01550 | Train Loss 1.1108 | Train Acc 0.6582 | Val Loss 1.1298 | Val Acc 0.6536\n",
      "Epoch 01560 | Train Loss 1.1106 | Train Acc 0.6578 | Val Loss 1.1319 | Val Acc 0.6520\n",
      "Epoch 01570 | Train Loss 1.1103 | Train Acc 0.6583 | Val Loss 1.1298 | Val Acc 0.6535\n",
      "Epoch 01580 | Train Loss 1.1109 | Train Acc 0.6584 | Val Loss 1.1321 | Val Acc 0.6527\n",
      "Epoch 01590 | Train Loss 1.1069 | Train Acc 0.6594 | Val Loss 1.1306 | Val Acc 0.6512\n",
      "Epoch 01600 | Train Loss 1.1085 | Train Acc 0.6593 | Val Loss 1.1266 | Val Acc 0.6543\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1600.pt'.\n",
      "Epoch 01610 | Train Loss 1.1084 | Train Acc 0.6590 | Val Loss 1.1273 | Val Acc 0.6515\n",
      "Epoch 01620 | Train Loss 1.1076 | Train Acc 0.6593 | Val Loss 1.1251 | Val Acc 0.6542\n",
      "Epoch 01630 | Train Loss 1.1068 | Train Acc 0.6598 | Val Loss 1.1265 | Val Acc 0.6544\n",
      "Epoch 01640 | Train Loss 1.1079 | Train Acc 0.6592 | Val Loss 1.1309 | Val Acc 0.6524\n",
      "Epoch 01650 | Train Loss 1.1079 | Train Acc 0.6590 | Val Loss 1.1283 | Val Acc 0.6516\n",
      "Epoch 01660 | Train Loss 1.1050 | Train Acc 0.6599 | Val Loss 1.1287 | Val Acc 0.6525\n",
      "Epoch 01670 | Train Loss 1.1053 | Train Acc 0.6598 | Val Loss 1.1232 | Val Acc 0.6544\n",
      "Epoch 01680 | Train Loss 1.1077 | Train Acc 0.6590 | Val Loss 1.1301 | Val Acc 0.6530\n",
      "Epoch 01690 | Train Loss 1.1057 | Train Acc 0.6593 | Val Loss 1.1269 | Val Acc 0.6538\n",
      "Epoch 01700 | Train Loss 1.1052 | Train Acc 0.6599 | Val Loss 1.1255 | Val Acc 0.6543\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1700.pt'.\n",
      "Epoch 01710 | Train Loss 1.1045 | Train Acc 0.6603 | Val Loss 1.1221 | Val Acc 0.6542\n",
      "Epoch 01720 | Train Loss 1.1040 | Train Acc 0.6604 | Val Loss 1.1235 | Val Acc 0.6541\n",
      "Epoch 01730 | Train Loss 1.1042 | Train Acc 0.6603 | Val Loss 1.1264 | Val Acc 0.6539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01740 | Train Loss 1.1027 | Train Acc 0.6599 | Val Loss 1.1239 | Val Acc 0.6546\n",
      "Epoch  1744: reducing learning rate of group 0 to 4.2188e-03.\n",
      "Epoch 01750 | Train Loss 1.1032 | Train Acc 0.6607 | Val Loss 1.1225 | Val Acc 0.6537\n",
      "Epoch 01760 | Train Loss 1.1028 | Train Acc 0.6599 | Val Loss 1.1215 | Val Acc 0.6539\n",
      "Epoch 01770 | Train Loss 1.1004 | Train Acc 0.6610 | Val Loss 1.1219 | Val Acc 0.6548\n",
      "Epoch 01780 | Train Loss 1.1013 | Train Acc 0.6608 | Val Loss 1.1216 | Val Acc 0.6546\n",
      "Epoch 01790 | Train Loss 1.1013 | Train Acc 0.6611 | Val Loss 1.1255 | Val Acc 0.6541\n",
      "Epoch 01800 | Train Loss 1.0990 | Train Acc 0.6615 | Val Loss 1.1205 | Val Acc 0.6540\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1800.pt'.\n",
      "Epoch 01810 | Train Loss 1.1019 | Train Acc 0.6610 | Val Loss 1.1253 | Val Acc 0.6547\n",
      "Epoch 01820 | Train Loss 1.0993 | Train Acc 0.6618 | Val Loss 1.1183 | Val Acc 0.6560\n",
      "Epoch 01830 | Train Loss 1.0997 | Train Acc 0.6615 | Val Loss 1.1221 | Val Acc 0.6546\n",
      "Epoch 01840 | Train Loss 1.1000 | Train Acc 0.6613 | Val Loss 1.1226 | Val Acc 0.6550\n",
      "Epoch 01850 | Train Loss 1.0995 | Train Acc 0.6614 | Val Loss 1.1211 | Val Acc 0.6557\n",
      "Epoch 01860 | Train Loss 1.0983 | Train Acc 0.6618 | Val Loss 1.1144 | Val Acc 0.6572\n",
      "Epoch 01870 | Train Loss 1.0973 | Train Acc 0.6617 | Val Loss 1.1174 | Val Acc 0.6556\n",
      "Epoch 01880 | Train Loss 1.0981 | Train Acc 0.6619 | Val Loss 1.1162 | Val Acc 0.6568\n",
      "Epoch 01890 | Train Loss 1.0986 | Train Acc 0.6619 | Val Loss 1.1190 | Val Acc 0.6557\n",
      "Epoch 01900 | Train Loss 1.0981 | Train Acc 0.6618 | Val Loss 1.1178 | Val Acc 0.6562\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_epoch_1900.pt'.\n",
      "Epoch 01910 | Train Loss 1.0977 | Train Acc 0.6619 | Val Loss 1.1184 | Val Acc 0.6555\n",
      "Epoch 01920 | Train Loss 1.0975 | Train Acc 0.6623 | Val Loss 1.1200 | Val Acc 0.6556\n",
      "Epoch 01930 | Train Loss 1.0970 | Train Acc 0.6624 | Val Loss 1.1178 | Val Acc 0.6578\n",
      "Epoch 01940 | Train Loss 1.0970 | Train Acc 0.6623 | Val Loss 1.1184 | Val Acc 0.6561\n",
      "Epoch  1949: reducing learning rate of group 0 to 3.1641e-03.\n",
      "Epoch 01950 | Train Loss 1.0958 | Train Acc 0.6623 | Val Loss 1.1155 | Val Acc 0.6571\n",
      "Epoch 01960 | Train Loss 1.0955 | Train Acc 0.6627 | Val Loss 1.1195 | Val Acc 0.6551\n",
      "Epoch 01970 | Train Loss 1.0944 | Train Acc 0.6630 | Val Loss 1.1167 | Val Acc 0.6571\n",
      "Epoch 01980 | Train Loss 1.0954 | Train Acc 0.6628 | Val Loss 1.1170 | Val Acc 0.6579\n",
      "Epoch 01990 | Train Loss 1.0937 | Train Acc 0.6633 | Val Loss 1.1149 | Val Acc 0.6562\n",
      "Model saved in './saved_models/graphsage_aminer/checkpoint_final.pt'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-f00718eed851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                              \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                              \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                              verbose=True)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "trainer.train(model=model,\n",
    "              n_epoch=2000,\n",
    "              save_dir=\"./saved_models/graphsage_aminer\",\n",
    "              eval_every=10,\n",
    "              save_after=100,\n",
    "              dropout=0.5,\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e640e4ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:44:25.007620Z",
     "start_time": "2021-05-06T10:44:24.993338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved_models/graphsage_aminer/checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e706fa36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:44:25.947342Z",
     "start_time": "2021-05-06T10:44:25.781119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.6677\n"
     ]
    }
   ],
   "source": [
    "logits, test_acc = trainer.inference(model)\n",
    "print(\"Test ACC: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9a7065",
   "metadata": {},
   "source": [
    "## RobustGCN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96edd116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T01:48:33.486507Z",
     "start_time": "2021-05-06T01:48:33.483457Z"
    }
   },
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33f85c58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:45:15.688194Z",
     "start_time": "2021-05-06T10:45:15.674915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 96548.\n",
      "RobustGCN(\n",
      "  (layers): ModuleList(\n",
      "    (0): RobustGCNConv(\n",
      "      (mean_conv): Linear(in_features=100, out_features=128, bias=True)\n",
      "      (var_conv): Linear(in_features=100, out_features=128, bias=True)\n",
      "    )\n",
      "    (1): RobustGCNConv(\n",
      "      (mean_conv): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (var_conv): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (2): RobustGCNConv(\n",
      "      (mean_conv): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (var_conv): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "    (3): RobustGCNConv(\n",
      "      (mean_conv): Linear(in_features=128, out_features=18, bias=True)\n",
      "      (var_conv): Linear(in_features=128, out_features=18, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.robustgcn import RobustGCN\n",
    "\n",
    "model = RobustGCN(in_features=num_features, out_features=num_classes, \n",
    "                  hidden_features=[128, 128, 128])\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cadfc29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:46:32.807845Z",
     "start_time": "2021-05-06T10:45:19.402770Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.model.trainer import Trainer\n",
    "from grb.utils.normalize import RobustGCNAdjNorm\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(dataset=dataset, \n",
    "                  optimizer=adam, \n",
    "                  loss=F.nll_loss,\n",
    "                  adj_norm_func=RobustGCNAdjNorm,\n",
    "                  lr_scheduler=True,\n",
    "                  early_stop=True,\n",
    "                  device='cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a15dcb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T03:41:52.874413Z",
     "start_time": "2021-05-06T03:24:11.426803Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 2.9001 | Train Acc 0.0351 | Val Loss 2.9000 | Val Acc 0.0342\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_0.pt'.\n",
      "Epoch 00020 | Train Loss 2.2813 | Train Acc 0.3303 | Val Loss 2.2832 | Val Acc 0.3264\n",
      "Epoch 00040 | Train Loss 1.8688 | Train Acc 0.4453 | Val Loss 1.8695 | Val Acc 0.4441\n",
      "Epoch 00060 | Train Loss 1.5769 | Train Acc 0.5349 | Val Loss 1.5804 | Val Acc 0.5328\n",
      "Epoch 00080 | Train Loss 1.4389 | Train Acc 0.5694 | Val Loss 1.4461 | Val Acc 0.5662\n",
      "Epoch 00100 | Train Loss 1.3806 | Train Acc 0.5857 | Val Loss 1.3899 | Val Acc 0.5830\n",
      "Epoch 00120 | Train Loss 1.3256 | Train Acc 0.5977 | Val Loss 1.3336 | Val Acc 0.5931\n",
      "Epoch 00140 | Train Loss 1.3007 | Train Acc 0.6035 | Val Loss 1.3094 | Val Acc 0.6004\n",
      "Epoch 00160 | Train Loss 1.2783 | Train Acc 0.6078 | Val Loss 1.2856 | Val Acc 0.6047\n",
      "Epoch 00180 | Train Loss 1.2639 | Train Acc 0.6119 | Val Loss 1.2747 | Val Acc 0.6079\n",
      "Epoch 00200 | Train Loss 1.2534 | Train Acc 0.6143 | Val Loss 1.2632 | Val Acc 0.6112\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_200.pt'.\n",
      "Epoch 00220 | Train Loss 1.2418 | Train Acc 0.6172 | Val Loss 1.2512 | Val Acc 0.6137\n",
      "Epoch 00240 | Train Loss 1.2323 | Train Acc 0.6202 | Val Loss 1.2444 | Val Acc 0.6164\n",
      "Epoch 00260 | Train Loss 1.2251 | Train Acc 0.6218 | Val Loss 1.2330 | Val Acc 0.6185\n",
      "Epoch 00280 | Train Loss 1.2159 | Train Acc 0.6245 | Val Loss 1.2279 | Val Acc 0.6208\n",
      "Epoch 00300 | Train Loss 1.2090 | Train Acc 0.6247 | Val Loss 1.2187 | Val Acc 0.6215\n",
      "Epoch 00320 | Train Loss 1.2020 | Train Acc 0.6268 | Val Loss 1.2127 | Val Acc 0.6232\n",
      "Epoch 00340 | Train Loss 1.1987 | Train Acc 0.6280 | Val Loss 1.2082 | Val Acc 0.6259\n",
      "Epoch 00360 | Train Loss 1.1917 | Train Acc 0.6295 | Val Loss 1.2008 | Val Acc 0.6281\n",
      "Epoch 00380 | Train Loss 1.1927 | Train Acc 0.6296 | Val Loss 1.2014 | Val Acc 0.6258\n",
      "Epoch 00400 | Train Loss 1.1833 | Train Acc 0.6314 | Val Loss 1.1955 | Val Acc 0.6272\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_400.pt'.\n",
      "Epoch 00420 | Train Loss 1.1813 | Train Acc 0.6320 | Val Loss 1.1897 | Val Acc 0.6268\n",
      "Epoch 00440 | Train Loss 1.1763 | Train Acc 0.6338 | Val Loss 1.1869 | Val Acc 0.6281\n",
      "Epoch 00460 | Train Loss 1.1724 | Train Acc 0.6348 | Val Loss 1.1830 | Val Acc 0.6295\n",
      "Epoch 00480 | Train Loss 1.1741 | Train Acc 0.6342 | Val Loss 1.1830 | Val Acc 0.6319\n",
      "Epoch 00500 | Train Loss 1.1670 | Train Acc 0.6362 | Val Loss 1.1771 | Val Acc 0.6326\n",
      "Epoch 00520 | Train Loss 1.1616 | Train Acc 0.6371 | Val Loss 1.1740 | Val Acc 0.6334\n",
      "Epoch 00540 | Train Loss 1.1673 | Train Acc 0.6360 | Val Loss 1.1758 | Val Acc 0.6308\n",
      "Epoch 00560 | Train Loss 1.1586 | Train Acc 0.6380 | Val Loss 1.1666 | Val Acc 0.6326\n",
      "Epoch 00580 | Train Loss 1.1565 | Train Acc 0.6386 | Val Loss 1.1651 | Val Acc 0.6331\n",
      "Epoch 00600 | Train Loss 1.1531 | Train Acc 0.6385 | Val Loss 1.1625 | Val Acc 0.6338\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_600.pt'.\n",
      "Epoch 00620 | Train Loss 1.1528 | Train Acc 0.6392 | Val Loss 1.1597 | Val Acc 0.6354\n",
      "Epoch 00640 | Train Loss 1.1463 | Train Acc 0.6416 | Val Loss 1.1578 | Val Acc 0.6365\n",
      "Epoch 00660 | Train Loss 1.1450 | Train Acc 0.6418 | Val Loss 1.1560 | Val Acc 0.6376\n",
      "Epoch 00680 | Train Loss 1.1441 | Train Acc 0.6422 | Val Loss 1.1541 | Val Acc 0.6376\n",
      "Epoch 00700 | Train Loss 1.1435 | Train Acc 0.6419 | Val Loss 1.1528 | Val Acc 0.6388\n",
      "Epoch 00720 | Train Loss 1.1392 | Train Acc 0.6427 | Val Loss 1.1479 | Val Acc 0.6390\n",
      "Epoch 00740 | Train Loss 1.1371 | Train Acc 0.6432 | Val Loss 1.1461 | Val Acc 0.6398\n",
      "Epoch 00760 | Train Loss 1.1354 | Train Acc 0.6443 | Val Loss 1.1474 | Val Acc 0.6393\n",
      "Epoch 00780 | Train Loss 1.1347 | Train Acc 0.6452 | Val Loss 1.1460 | Val Acc 0.6406\n",
      "Epoch 00800 | Train Loss 1.1334 | Train Acc 0.6453 | Val Loss 1.1419 | Val Acc 0.6396\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_800.pt'.\n",
      "Epoch 00820 | Train Loss 1.1303 | Train Acc 0.6453 | Val Loss 1.1388 | Val Acc 0.6420\n",
      "Epoch 00840 | Train Loss 1.1301 | Train Acc 0.6460 | Val Loss 1.1395 | Val Acc 0.6418\n",
      "Epoch 00860 | Train Loss 1.1273 | Train Acc 0.6466 | Val Loss 1.1374 | Val Acc 0.6430\n",
      "Epoch 00880 | Train Loss 1.1253 | Train Acc 0.6468 | Val Loss 1.1371 | Val Acc 0.6429\n",
      "Epoch 00900 | Train Loss 1.1268 | Train Acc 0.6460 | Val Loss 1.1377 | Val Acc 0.6410\n",
      "Epoch 00920 | Train Loss 1.1248 | Train Acc 0.6472 | Val Loss 1.1341 | Val Acc 0.6441\n",
      "Epoch 00940 | Train Loss 1.1239 | Train Acc 0.6472 | Val Loss 1.1346 | Val Acc 0.6443\n",
      "Epoch 00960 | Train Loss 1.1215 | Train Acc 0.6480 | Val Loss 1.1335 | Val Acc 0.6443\n",
      "Epoch 00980 | Train Loss 1.1173 | Train Acc 0.6496 | Val Loss 1.1294 | Val Acc 0.6461\n",
      "Epoch 01000 | Train Loss 1.1193 | Train Acc 0.6494 | Val Loss 1.1300 | Val Acc 0.6458\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_1000.pt'.\n",
      "Epoch 01020 | Train Loss 1.1150 | Train Acc 0.6500 | Val Loss 1.1259 | Val Acc 0.6464\n",
      "Epoch 01040 | Train Loss 1.1155 | Train Acc 0.6500 | Val Loss 1.1251 | Val Acc 0.6449\n",
      "Epoch 01060 | Train Loss 1.1136 | Train Acc 0.6505 | Val Loss 1.1248 | Val Acc 0.6468\n",
      "Epoch 01080 | Train Loss 1.1138 | Train Acc 0.6503 | Val Loss 1.1267 | Val Acc 0.6462\n",
      "Epoch 01100 | Train Loss 1.1143 | Train Acc 0.6503 | Val Loss 1.1254 | Val Acc 0.6455\n",
      "Epoch 01120 | Train Loss 1.1108 | Train Acc 0.6514 | Val Loss 1.1237 | Val Acc 0.6455\n",
      "Epoch 01140 | Train Loss 1.1097 | Train Acc 0.6513 | Val Loss 1.1228 | Val Acc 0.6469\n",
      "Epoch 01160 | Train Loss 1.1081 | Train Acc 0.6521 | Val Loss 1.1230 | Val Acc 0.6455\n",
      "Epoch 01180 | Train Loss 1.1079 | Train Acc 0.6517 | Val Loss 1.1195 | Val Acc 0.6474\n",
      "Epoch 01200 | Train Loss 1.1066 | Train Acc 0.6527 | Val Loss 1.1193 | Val Acc 0.6458\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_1200.pt'.\n",
      "Epoch 01220 | Train Loss 1.1047 | Train Acc 0.6525 | Val Loss 1.1147 | Val Acc 0.6490\n",
      "Epoch 01240 | Train Loss 1.1062 | Train Acc 0.6520 | Val Loss 1.1177 | Val Acc 0.6465\n",
      "Epoch 01260 | Train Loss 1.1104 | Train Acc 0.6505 | Val Loss 1.1225 | Val Acc 0.6453\n",
      "Epoch 01280 | Train Loss 1.1035 | Train Acc 0.6530 | Val Loss 1.1143 | Val Acc 0.6490\n",
      "Epoch 01300 | Train Loss 1.1017 | Train Acc 0.6537 | Val Loss 1.1109 | Val Acc 0.6500\n",
      "Epoch 01320 | Train Loss 1.1012 | Train Acc 0.6534 | Val Loss 1.1125 | Val Acc 0.6489\n",
      "Epoch 01340 | Train Loss 1.1008 | Train Acc 0.6543 | Val Loss 1.1097 | Val Acc 0.6499\n",
      "Epoch 01360 | Train Loss 1.1039 | Train Acc 0.6538 | Val Loss 1.1151 | Val Acc 0.6494\n",
      "Epoch 01380 | Train Loss 1.0995 | Train Acc 0.6544 | Val Loss 1.1120 | Val Acc 0.6498\n",
      "Epoch 01400 | Train Loss 1.0998 | Train Acc 0.6548 | Val Loss 1.1101 | Val Acc 0.6504\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_1400.pt'.\n",
      "Epoch 01420 | Train Loss 1.0986 | Train Acc 0.6543 | Val Loss 1.1095 | Val Acc 0.6508\n",
      "Epoch 01440 | Train Loss 1.0955 | Train Acc 0.6547 | Val Loss 1.1082 | Val Acc 0.6517\n",
      "Epoch 01460 | Train Loss 1.0986 | Train Acc 0.6546 | Val Loss 1.1115 | Val Acc 0.6499\n",
      "Epoch  1476: reducing learning rate of group 0 to 7.5000e-03.\n",
      "Epoch 01480 | Train Loss 1.0957 | Train Acc 0.6551 | Val Loss 1.1090 | Val Acc 0.6498\n",
      "Epoch 01500 | Train Loss 1.0939 | Train Acc 0.6563 | Val Loss 1.1041 | Val Acc 0.6532\n",
      "Epoch 01520 | Train Loss 1.0924 | Train Acc 0.6562 | Val Loss 1.1036 | Val Acc 0.6523\n",
      "Epoch 01540 | Train Loss 1.0923 | Train Acc 0.6557 | Val Loss 1.1038 | Val Acc 0.6523\n",
      "Epoch 01560 | Train Loss 1.0928 | Train Acc 0.6563 | Val Loss 1.1048 | Val Acc 0.6526\n",
      "Epoch 01580 | Train Loss 1.0931 | Train Acc 0.6565 | Val Loss 1.1045 | Val Acc 0.6522\n",
      "Epoch  1601: reducing learning rate of group 0 to 5.6250e-03.\n",
      "Epoch 01600 | Train Loss 1.0910 | Train Acc 0.6570 | Val Loss 1.1028 | Val Acc 0.6536\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_1600.pt'.\n",
      "Epoch 01620 | Train Loss 1.0903 | Train Acc 0.6564 | Val Loss 1.1037 | Val Acc 0.6514\n",
      "Epoch 01640 | Train Loss 1.0892 | Train Acc 0.6566 | Val Loss 1.1004 | Val Acc 0.6510\n",
      "Training early stopped.\n",
      "Model saved in './saved_models/robustgcn_aminer/checkpoint_epoch_1650_early_stopped.pt'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([tensor(0.0351, device='cuda:3'),\n",
       "  tensor(0.3303, device='cuda:3'),\n",
       "  tensor(0.4453, device='cuda:3'),\n",
       "  tensor(0.5349, device='cuda:3'),\n",
       "  tensor(0.5694, device='cuda:3'),\n",
       "  tensor(0.5857, device='cuda:3'),\n",
       "  tensor(0.5977, device='cuda:3'),\n",
       "  tensor(0.6035, device='cuda:3'),\n",
       "  tensor(0.6078, device='cuda:3'),\n",
       "  tensor(0.6119, device='cuda:3'),\n",
       "  tensor(0.6143, device='cuda:3'),\n",
       "  tensor(0.6172, device='cuda:3'),\n",
       "  tensor(0.6202, device='cuda:3'),\n",
       "  tensor(0.6218, device='cuda:3'),\n",
       "  tensor(0.6245, device='cuda:3'),\n",
       "  tensor(0.6247, device='cuda:3'),\n",
       "  tensor(0.6268, device='cuda:3'),\n",
       "  tensor(0.6280, device='cuda:3'),\n",
       "  tensor(0.6295, device='cuda:3'),\n",
       "  tensor(0.6296, device='cuda:3'),\n",
       "  tensor(0.6314, device='cuda:3'),\n",
       "  tensor(0.6320, device='cuda:3'),\n",
       "  tensor(0.6338, device='cuda:3'),\n",
       "  tensor(0.6348, device='cuda:3'),\n",
       "  tensor(0.6342, device='cuda:3'),\n",
       "  tensor(0.6362, device='cuda:3'),\n",
       "  tensor(0.6371, device='cuda:3'),\n",
       "  tensor(0.6360, device='cuda:3'),\n",
       "  tensor(0.6380, device='cuda:3'),\n",
       "  tensor(0.6386, device='cuda:3'),\n",
       "  tensor(0.6385, device='cuda:3'),\n",
       "  tensor(0.6392, device='cuda:3'),\n",
       "  tensor(0.6416, device='cuda:3'),\n",
       "  tensor(0.6418, device='cuda:3'),\n",
       "  tensor(0.6422, device='cuda:3'),\n",
       "  tensor(0.6419, device='cuda:3'),\n",
       "  tensor(0.6427, device='cuda:3'),\n",
       "  tensor(0.6432, device='cuda:3'),\n",
       "  tensor(0.6443, device='cuda:3'),\n",
       "  tensor(0.6452, device='cuda:3'),\n",
       "  tensor(0.6453, device='cuda:3'),\n",
       "  tensor(0.6453, device='cuda:3'),\n",
       "  tensor(0.6460, device='cuda:3'),\n",
       "  tensor(0.6466, device='cuda:3'),\n",
       "  tensor(0.6468, device='cuda:3'),\n",
       "  tensor(0.6460, device='cuda:3'),\n",
       "  tensor(0.6472, device='cuda:3'),\n",
       "  tensor(0.6472, device='cuda:3'),\n",
       "  tensor(0.6480, device='cuda:3'),\n",
       "  tensor(0.6496, device='cuda:3'),\n",
       "  tensor(0.6494, device='cuda:3'),\n",
       "  tensor(0.6500, device='cuda:3'),\n",
       "  tensor(0.6500, device='cuda:3'),\n",
       "  tensor(0.6505, device='cuda:3'),\n",
       "  tensor(0.6503, device='cuda:3'),\n",
       "  tensor(0.6503, device='cuda:3'),\n",
       "  tensor(0.6514, device='cuda:3'),\n",
       "  tensor(0.6513, device='cuda:3'),\n",
       "  tensor(0.6521, device='cuda:3'),\n",
       "  tensor(0.6517, device='cuda:3'),\n",
       "  tensor(0.6527, device='cuda:3'),\n",
       "  tensor(0.6525, device='cuda:3'),\n",
       "  tensor(0.6520, device='cuda:3'),\n",
       "  tensor(0.6505, device='cuda:3'),\n",
       "  tensor(0.6530, device='cuda:3'),\n",
       "  tensor(0.6537, device='cuda:3'),\n",
       "  tensor(0.6534, device='cuda:3'),\n",
       "  tensor(0.6543, device='cuda:3'),\n",
       "  tensor(0.6538, device='cuda:3'),\n",
       "  tensor(0.6544, device='cuda:3'),\n",
       "  tensor(0.6548, device='cuda:3'),\n",
       "  tensor(0.6543, device='cuda:3'),\n",
       "  tensor(0.6547, device='cuda:3'),\n",
       "  tensor(0.6546, device='cuda:3'),\n",
       "  tensor(0.6551, device='cuda:3'),\n",
       "  tensor(0.6563, device='cuda:3'),\n",
       "  tensor(0.6562, device='cuda:3'),\n",
       "  tensor(0.6557, device='cuda:3'),\n",
       "  tensor(0.6563, device='cuda:3'),\n",
       "  tensor(0.6565, device='cuda:3'),\n",
       "  tensor(0.6570, device='cuda:3'),\n",
       "  tensor(0.6564, device='cuda:3'),\n",
       "  tensor(0.6566, device='cuda:3')],\n",
       " [tensor(0.0342, device='cuda:3'),\n",
       "  tensor(0.3264, device='cuda:3'),\n",
       "  tensor(0.4441, device='cuda:3'),\n",
       "  tensor(0.5328, device='cuda:3'),\n",
       "  tensor(0.5662, device='cuda:3'),\n",
       "  tensor(0.5830, device='cuda:3'),\n",
       "  tensor(0.5931, device='cuda:3'),\n",
       "  tensor(0.6004, device='cuda:3'),\n",
       "  tensor(0.6047, device='cuda:3'),\n",
       "  tensor(0.6079, device='cuda:3'),\n",
       "  tensor(0.6112, device='cuda:3'),\n",
       "  tensor(0.6137, device='cuda:3'),\n",
       "  tensor(0.6164, device='cuda:3'),\n",
       "  tensor(0.6185, device='cuda:3'),\n",
       "  tensor(0.6208, device='cuda:3'),\n",
       "  tensor(0.6215, device='cuda:3'),\n",
       "  tensor(0.6232, device='cuda:3'),\n",
       "  tensor(0.6259, device='cuda:3'),\n",
       "  tensor(0.6281, device='cuda:3'),\n",
       "  tensor(0.6258, device='cuda:3'),\n",
       "  tensor(0.6272, device='cuda:3'),\n",
       "  tensor(0.6268, device='cuda:3'),\n",
       "  tensor(0.6281, device='cuda:3'),\n",
       "  tensor(0.6295, device='cuda:3'),\n",
       "  tensor(0.6319, device='cuda:3'),\n",
       "  tensor(0.6326, device='cuda:3'),\n",
       "  tensor(0.6334, device='cuda:3'),\n",
       "  tensor(0.6308, device='cuda:3'),\n",
       "  tensor(0.6326, device='cuda:3'),\n",
       "  tensor(0.6331, device='cuda:3'),\n",
       "  tensor(0.6338, device='cuda:3'),\n",
       "  tensor(0.6354, device='cuda:3'),\n",
       "  tensor(0.6365, device='cuda:3'),\n",
       "  tensor(0.6376, device='cuda:3'),\n",
       "  tensor(0.6376, device='cuda:3'),\n",
       "  tensor(0.6388, device='cuda:3'),\n",
       "  tensor(0.6390, device='cuda:3'),\n",
       "  tensor(0.6398, device='cuda:3'),\n",
       "  tensor(0.6393, device='cuda:3'),\n",
       "  tensor(0.6406, device='cuda:3'),\n",
       "  tensor(0.6396, device='cuda:3'),\n",
       "  tensor(0.6420, device='cuda:3'),\n",
       "  tensor(0.6418, device='cuda:3'),\n",
       "  tensor(0.6430, device='cuda:3'),\n",
       "  tensor(0.6429, device='cuda:3'),\n",
       "  tensor(0.6410, device='cuda:3'),\n",
       "  tensor(0.6441, device='cuda:3'),\n",
       "  tensor(0.6443, device='cuda:3'),\n",
       "  tensor(0.6443, device='cuda:3'),\n",
       "  tensor(0.6461, device='cuda:3'),\n",
       "  tensor(0.6458, device='cuda:3'),\n",
       "  tensor(0.6464, device='cuda:3'),\n",
       "  tensor(0.6449, device='cuda:3'),\n",
       "  tensor(0.6468, device='cuda:3'),\n",
       "  tensor(0.6462, device='cuda:3'),\n",
       "  tensor(0.6455, device='cuda:3'),\n",
       "  tensor(0.6455, device='cuda:3'),\n",
       "  tensor(0.6469, device='cuda:3'),\n",
       "  tensor(0.6455, device='cuda:3'),\n",
       "  tensor(0.6474, device='cuda:3'),\n",
       "  tensor(0.6458, device='cuda:3'),\n",
       "  tensor(0.6490, device='cuda:3'),\n",
       "  tensor(0.6465, device='cuda:3'),\n",
       "  tensor(0.6453, device='cuda:3'),\n",
       "  tensor(0.6490, device='cuda:3'),\n",
       "  tensor(0.6500, device='cuda:3'),\n",
       "  tensor(0.6489, device='cuda:3'),\n",
       "  tensor(0.6499, device='cuda:3'),\n",
       "  tensor(0.6494, device='cuda:3'),\n",
       "  tensor(0.6498, device='cuda:3'),\n",
       "  tensor(0.6504, device='cuda:3'),\n",
       "  tensor(0.6508, device='cuda:3'),\n",
       "  tensor(0.6517, device='cuda:3'),\n",
       "  tensor(0.6499, device='cuda:3'),\n",
       "  tensor(0.6498, device='cuda:3'),\n",
       "  tensor(0.6532, device='cuda:3'),\n",
       "  tensor(0.6523, device='cuda:3'),\n",
       "  tensor(0.6523, device='cuda:3'),\n",
       "  tensor(0.6526, device='cuda:3'),\n",
       "  tensor(0.6522, device='cuda:3'),\n",
       "  tensor(0.6536, device='cuda:3'),\n",
       "  tensor(0.6514, device='cuda:3'),\n",
       "  tensor(0.6510, device='cuda:3')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(model=model,\n",
    "              n_epoch=5000,\n",
    "              save_dir=\"./saved_models/robustgcn_aminer\",\n",
    "              eval_every=20,\n",
    "              save_after=200,\n",
    "              dropout=0.5,\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de207136",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:46:54.243550Z",
     "start_time": "2021-05-06T10:46:54.203235Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_models/robustgcn_aminer/checkpoint_final.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c1c459a8d213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./saved_models/robustgcn_aminer/checkpoint_final.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './saved_models/robustgcn_aminer/checkpoint_final.pt'"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved_models/robustgcn_aminer/checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cee6cacb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:46:55.095690Z",
     "start_time": "2021-05-06T10:46:54.694201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.6801\n"
     ]
    }
   ],
   "source": [
    "logits, test_acc = trainer.inference(model)\n",
    "print(\"Test ACC: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "890a1a59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:48:05.618952Z",
     "start_time": "2021-05-06T10:48:05.505110Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-586d5fb929ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "np.where(adj.data > 0 & adj.data != 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "001d828d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:50:31.350348Z",
     "start_time": "2021-05-06T10:50:30.656154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,       1,       2, ..., 5757151, 5757152, 5757153])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where((adj.data > 0) * (adj.data == 1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6497db00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T10:50:46.381648Z",
     "start_time": "2021-05-06T10:50:45.739726Z"
    }
   },
   "outputs": [],
   "source": [
    "adj.data[np.where((adj.data > 0) * (adj.data == 1))[0]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975dfe2",
   "metadata": {},
   "source": [
    "## APPNP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe11f1",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79cd63a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:43:24.405573Z",
     "start_time": "2021-05-06T05:43:24.400156Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 15250.\n",
      "APPNP(\n",
      "  (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
      "  (linear2): Linear(in_features=128, out_features=18, bias=True)\n",
      "  (dropout): SparseEdgeDrop()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from grb.model.appnp import APPNP\n",
    "\n",
    "model = APPNP(in_features=num_features, out_features=num_classes, hidden_features=128, alpha=0.01, k=10)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34f1069d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T05:43:33.895802Z",
     "start_time": "2021-05-06T05:43:24.407331Z"
    }
   },
   "outputs": [],
   "source": [
    "from grb.model.trainer import Trainer\n",
    "from grb.utils.normalize import GCNAdjNorm\n",
    "\n",
    "adam = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "trainer = Trainer(dataset=dataset, \n",
    "                  optimizer=adam, \n",
    "                  loss=F.nll_loss,\n",
    "                  adj_norm_func=GCNAdjNorm,\n",
    "                  lr_scheduler=True,\n",
    "                  early_stop=None,\n",
    "                  device='cuda:3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c6ae0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T06:00:56.247128Z",
     "start_time": "2021-05-06T05:43:33.897748Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 2.8816 | Train Acc 0.0964 | Val Loss 2.8816 | Val Acc 0.0985\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_0.pt'.\n",
      "Epoch 00020 | Train Loss 2.5242 | Train Acc 0.2423 | Val Loss 2.5274 | Val Acc 0.2377\n",
      "Epoch 00040 | Train Loss 2.2119 | Train Acc 0.3471 | Val Loss 2.2148 | Val Acc 0.3449\n",
      "Epoch 00060 | Train Loss 1.8402 | Train Acc 0.4924 | Val Loss 1.8431 | Val Acc 0.4898\n",
      "Epoch 00080 | Train Loss 1.6149 | Train Acc 0.5458 | Val Loss 1.6192 | Val Acc 0.5431\n",
      "Epoch 00100 | Train Loss 1.4967 | Train Acc 0.5763 | Val Loss 1.5019 | Val Acc 0.5728\n",
      "Epoch 00120 | Train Loss 1.4406 | Train Acc 0.5899 | Val Loss 1.4465 | Val Acc 0.5869\n",
      "Epoch 00140 | Train Loss 1.3977 | Train Acc 0.5984 | Val Loss 1.4017 | Val Acc 0.5946\n",
      "Epoch 00160 | Train Loss 1.3709 | Train Acc 0.6065 | Val Loss 1.3769 | Val Acc 0.6025\n",
      "Epoch 00180 | Train Loss 1.3546 | Train Acc 0.6087 | Val Loss 1.3601 | Val Acc 0.6060\n",
      "Epoch 00200 | Train Loss 1.3394 | Train Acc 0.6124 | Val Loss 1.3421 | Val Acc 0.6074\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_200.pt'.\n",
      "Epoch 00220 | Train Loss 1.3284 | Train Acc 0.6135 | Val Loss 1.3309 | Val Acc 0.6092\n",
      "Epoch 00240 | Train Loss 1.3181 | Train Acc 0.6165 | Val Loss 1.3217 | Val Acc 0.6128\n",
      "Epoch 00260 | Train Loss 1.3078 | Train Acc 0.6169 | Val Loss 1.3119 | Val Acc 0.6142\n",
      "Epoch 00280 | Train Loss 1.3051 | Train Acc 0.6187 | Val Loss 1.3071 | Val Acc 0.6166\n",
      "Epoch 00300 | Train Loss 1.2976 | Train Acc 0.6202 | Val Loss 1.3007 | Val Acc 0.6169\n",
      "Epoch 00320 | Train Loss 1.2919 | Train Acc 0.6206 | Val Loss 1.2954 | Val Acc 0.6170\n",
      "Epoch 00340 | Train Loss 1.2865 | Train Acc 0.6233 | Val Loss 1.2943 | Val Acc 0.6182\n",
      "Epoch 00360 | Train Loss 1.2839 | Train Acc 0.6236 | Val Loss 1.2903 | Val Acc 0.6202\n",
      "Epoch 00380 | Train Loss 1.2781 | Train Acc 0.6247 | Val Loss 1.2832 | Val Acc 0.6215\n",
      "Epoch 00400 | Train Loss 1.2754 | Train Acc 0.6242 | Val Loss 1.2783 | Val Acc 0.6210\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_400.pt'.\n",
      "Epoch 00420 | Train Loss 1.2708 | Train Acc 0.6247 | Val Loss 1.2766 | Val Acc 0.6219\n",
      "Epoch 00440 | Train Loss 1.2691 | Train Acc 0.6246 | Val Loss 1.2762 | Val Acc 0.6203\n",
      "Epoch 00460 | Train Loss 1.2665 | Train Acc 0.6269 | Val Loss 1.2751 | Val Acc 0.6223\n",
      "Epoch 00480 | Train Loss 1.2640 | Train Acc 0.6262 | Val Loss 1.2682 | Val Acc 0.6239\n",
      "Epoch 00500 | Train Loss 1.2626 | Train Acc 0.6276 | Val Loss 1.2714 | Val Acc 0.6231\n",
      "Epoch 00520 | Train Loss 1.2593 | Train Acc 0.6278 | Val Loss 1.2659 | Val Acc 0.6243\n",
      "Epoch 00540 | Train Loss 1.2567 | Train Acc 0.6279 | Val Loss 1.2645 | Val Acc 0.6255\n",
      "Epoch 00560 | Train Loss 1.2577 | Train Acc 0.6293 | Val Loss 1.2680 | Val Acc 0.6259\n",
      "Epoch 00580 | Train Loss 1.2514 | Train Acc 0.6292 | Val Loss 1.2609 | Val Acc 0.6260\n",
      "Epoch 00600 | Train Loss 1.2525 | Train Acc 0.6286 | Val Loss 1.2614 | Val Acc 0.6255\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_600.pt'.\n",
      "Epoch 00620 | Train Loss 1.2491 | Train Acc 0.6305 | Val Loss 1.2544 | Val Acc 0.6268\n",
      "Epoch 00640 | Train Loss 1.2492 | Train Acc 0.6280 | Val Loss 1.2557 | Val Acc 0.6242\n",
      "Epoch 00660 | Train Loss 1.2473 | Train Acc 0.6313 | Val Loss 1.2555 | Val Acc 0.6279\n",
      "Epoch 00680 | Train Loss 1.2471 | Train Acc 0.6307 | Val Loss 1.2521 | Val Acc 0.6275\n",
      "Epoch 00700 | Train Loss 1.2445 | Train Acc 0.6300 | Val Loss 1.2504 | Val Acc 0.6280\n",
      "Epoch 00720 | Train Loss 1.2442 | Train Acc 0.6316 | Val Loss 1.2503 | Val Acc 0.6276\n",
      "Epoch 00740 | Train Loss 1.2426 | Train Acc 0.6316 | Val Loss 1.2552 | Val Acc 0.6264\n",
      "Epoch 00760 | Train Loss 1.2388 | Train Acc 0.6334 | Val Loss 1.2492 | Val Acc 0.6300\n",
      "Epoch 00780 | Train Loss 1.2417 | Train Acc 0.6317 | Val Loss 1.2433 | Val Acc 0.6289\n",
      "Epoch   793: reducing learning rate of group 0 to 7.5000e-03.\n",
      "Epoch 00800 | Train Loss 1.2382 | Train Acc 0.6319 | Val Loss 1.2430 | Val Acc 0.6287\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_800.pt'.\n",
      "Epoch 00820 | Train Loss 1.2366 | Train Acc 0.6323 | Val Loss 1.2468 | Val Acc 0.6284\n",
      "Epoch 00840 | Train Loss 1.2353 | Train Acc 0.6329 | Val Loss 1.2485 | Val Acc 0.6276\n",
      "Epoch 00860 | Train Loss 1.2350 | Train Acc 0.6336 | Val Loss 1.2475 | Val Acc 0.6279\n",
      "Epoch 00880 | Train Loss 1.2361 | Train Acc 0.6350 | Val Loss 1.2396 | Val Acc 0.6327\n",
      "Epoch 00900 | Train Loss 1.2311 | Train Acc 0.6348 | Val Loss 1.2387 | Val Acc 0.6302\n",
      "Epoch 00920 | Train Loss 1.2335 | Train Acc 0.6339 | Val Loss 1.2427 | Val Acc 0.6307\n",
      "Epoch 00940 | Train Loss 1.2317 | Train Acc 0.6339 | Val Loss 1.2358 | Val Acc 0.6312\n",
      "Epoch   956: reducing learning rate of group 0 to 5.6250e-03.\n",
      "Epoch 00960 | Train Loss 1.2289 | Train Acc 0.6348 | Val Loss 1.2339 | Val Acc 0.6324\n",
      "Epoch 00980 | Train Loss 1.2321 | Train Acc 0.6337 | Val Loss 1.2416 | Val Acc 0.6292\n",
      "Epoch 01000 | Train Loss 1.2318 | Train Acc 0.6342 | Val Loss 1.2422 | Val Acc 0.6298\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_1000.pt'.\n",
      "Epoch 01020 | Train Loss 1.2299 | Train Acc 0.6350 | Val Loss 1.2390 | Val Acc 0.6303\n",
      "Epoch 01040 | Train Loss 1.2286 | Train Acc 0.6333 | Val Loss 1.2415 | Val Acc 0.6296\n",
      "Epoch 01060 | Train Loss 1.2301 | Train Acc 0.6338 | Val Loss 1.2390 | Val Acc 0.6296\n",
      "Epoch 01080 | Train Loss 1.2287 | Train Acc 0.6355 | Val Loss 1.2373 | Val Acc 0.6316\n",
      "Epoch 01100 | Train Loss 1.2271 | Train Acc 0.6346 | Val Loss 1.2339 | Val Acc 0.6326\n",
      "Epoch 01120 | Train Loss 1.2281 | Train Acc 0.6348 | Val Loss 1.2359 | Val Acc 0.6318\n",
      "Epoch 01140 | Train Loss 1.2275 | Train Acc 0.6350 | Val Loss 1.2382 | Val Acc 0.6307\n",
      "Epoch 01160 | Train Loss 1.2232 | Train Acc 0.6362 | Val Loss 1.2316 | Val Acc 0.6321\n",
      "Epoch 01180 | Train Loss 1.2257 | Train Acc 0.6349 | Val Loss 1.2345 | Val Acc 0.6311\n",
      "Epoch 01200 | Train Loss 1.2259 | Train Acc 0.6343 | Val Loss 1.2353 | Val Acc 0.6309\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_1200.pt'.\n",
      "Epoch 01220 | Train Loss 1.2260 | Train Acc 0.6357 | Val Loss 1.2340 | Val Acc 0.6331\n",
      "Epoch  1241: reducing learning rate of group 0 to 4.2188e-03.\n",
      "Epoch 01240 | Train Loss 1.2256 | Train Acc 0.6357 | Val Loss 1.2329 | Val Acc 0.6324\n",
      "Epoch 01260 | Train Loss 1.2262 | Train Acc 0.6354 | Val Loss 1.2367 | Val Acc 0.6302\n",
      "Epoch 01280 | Train Loss 1.2232 | Train Acc 0.6362 | Val Loss 1.2316 | Val Acc 0.6317\n",
      "Epoch 01300 | Train Loss 1.2241 | Train Acc 0.6369 | Val Loss 1.2359 | Val Acc 0.6335\n",
      "Epoch 01320 | Train Loss 1.2221 | Train Acc 0.6357 | Val Loss 1.2299 | Val Acc 0.6314\n",
      "Epoch  1338: reducing learning rate of group 0 to 3.1641e-03.\n",
      "Epoch 01340 | Train Loss 1.2230 | Train Acc 0.6369 | Val Loss 1.2337 | Val Acc 0.6329\n",
      "Epoch 01360 | Train Loss 1.2241 | Train Acc 0.6355 | Val Loss 1.2324 | Val Acc 0.6304\n",
      "Epoch 01380 | Train Loss 1.2220 | Train Acc 0.6368 | Val Loss 1.2354 | Val Acc 0.6324\n",
      "Epoch 01400 | Train Loss 1.2212 | Train Acc 0.6365 | Val Loss 1.2296 | Val Acc 0.6332\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_1400.pt'.\n",
      "Epoch 01420 | Train Loss 1.2224 | Train Acc 0.6361 | Val Loss 1.2308 | Val Acc 0.6326\n",
      "Epoch 01440 | Train Loss 1.2206 | Train Acc 0.6377 | Val Loss 1.2279 | Val Acc 0.6357\n",
      "Epoch 01460 | Train Loss 1.2197 | Train Acc 0.6356 | Val Loss 1.2337 | Val Acc 0.6328\n",
      "Epoch 01480 | Train Loss 1.2210 | Train Acc 0.6368 | Val Loss 1.2306 | Val Acc 0.6331\n",
      "Epoch 01500 | Train Loss 1.2195 | Train Acc 0.6366 | Val Loss 1.2289 | Val Acc 0.6338\n",
      "Epoch 01520 | Train Loss 1.2196 | Train Acc 0.6375 | Val Loss 1.2253 | Val Acc 0.6352\n",
      "Epoch 01540 | Train Loss 1.2213 | Train Acc 0.6363 | Val Loss 1.2321 | Val Acc 0.6301\n",
      "Epoch 01560 | Train Loss 1.2211 | Train Acc 0.6377 | Val Loss 1.2301 | Val Acc 0.6342\n",
      "Epoch  1565: reducing learning rate of group 0 to 2.3730e-03.\n",
      "Epoch 01580 | Train Loss 1.2219 | Train Acc 0.6374 | Val Loss 1.2350 | Val Acc 0.6327\n",
      "Epoch 01600 | Train Loss 1.2175 | Train Acc 0.6375 | Val Loss 1.2238 | Val Acc 0.6341\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_1600.pt'.\n",
      "Epoch  1616: reducing learning rate of group 0 to 1.7798e-03.\n",
      "Epoch 01620 | Train Loss 1.2178 | Train Acc 0.6369 | Val Loss 1.2300 | Val Acc 0.6331\n",
      "Epoch 01640 | Train Loss 1.2170 | Train Acc 0.6376 | Val Loss 1.2248 | Val Acc 0.6341\n",
      "Epoch 01660 | Train Loss 1.2206 | Train Acc 0.6366 | Val Loss 1.2280 | Val Acc 0.6317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01680 | Train Loss 1.2178 | Train Acc 0.6377 | Val Loss 1.2295 | Val Acc 0.6344\n",
      "Epoch 01700 | Train Loss 1.2199 | Train Acc 0.6368 | Val Loss 1.2310 | Val Acc 0.6321\n",
      "Epoch 01720 | Train Loss 1.2173 | Train Acc 0.6381 | Val Loss 1.2241 | Val Acc 0.6327\n",
      "Epoch 01740 | Train Loss 1.2177 | Train Acc 0.6374 | Val Loss 1.2262 | Val Acc 0.6340\n",
      "Epoch  1750: reducing learning rate of group 0 to 1.3348e-03.\n",
      "Epoch 01760 | Train Loss 1.2151 | Train Acc 0.6379 | Val Loss 1.2268 | Val Acc 0.6333\n",
      "Epoch 01780 | Train Loss 1.2179 | Train Acc 0.6382 | Val Loss 1.2284 | Val Acc 0.6350\n",
      "Epoch  1801: reducing learning rate of group 0 to 1.0011e-03.\n",
      "Epoch 01800 | Train Loss 1.2191 | Train Acc 0.6371 | Val Loss 1.2285 | Val Acc 0.6335\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_1800.pt'.\n",
      "Epoch 01820 | Train Loss 1.2155 | Train Acc 0.6371 | Val Loss 1.2236 | Val Acc 0.6338\n",
      "Epoch 01840 | Train Loss 1.2164 | Train Acc 0.6363 | Val Loss 1.2259 | Val Acc 0.6322\n",
      "Epoch  1852: reducing learning rate of group 0 to 7.5085e-04.\n",
      "Epoch 01860 | Train Loss 1.2186 | Train Acc 0.6375 | Val Loss 1.2242 | Val Acc 0.6325\n",
      "Epoch 01880 | Train Loss 1.2168 | Train Acc 0.6388 | Val Loss 1.2254 | Val Acc 0.6356\n",
      "Epoch 01900 | Train Loss 1.2179 | Train Acc 0.6380 | Val Loss 1.2231 | Val Acc 0.6354\n",
      "Epoch  1903: reducing learning rate of group 0 to 5.6314e-04.\n",
      "Epoch 01920 | Train Loss 1.2175 | Train Acc 0.6367 | Val Loss 1.2259 | Val Acc 0.6344\n",
      "Epoch 01940 | Train Loss 1.2161 | Train Acc 0.6387 | Val Loss 1.2296 | Val Acc 0.6344\n",
      "Epoch 01960 | Train Loss 1.2186 | Train Acc 0.6376 | Val Loss 1.2254 | Val Acc 0.6338\n",
      "Epoch 01980 | Train Loss 1.2195 | Train Acc 0.6382 | Val Loss 1.2273 | Val Acc 0.6347\n",
      "Epoch  1987: reducing learning rate of group 0 to 4.2235e-04.\n",
      "Epoch 02000 | Train Loss 1.2196 | Train Acc 0.6374 | Val Loss 1.2257 | Val Acc 0.6348\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_2000.pt'.\n",
      "Epoch 02020 | Train Loss 1.2176 | Train Acc 0.6382 | Val Loss 1.2230 | Val Acc 0.6351\n",
      "Epoch  2038: reducing learning rate of group 0 to 3.1676e-04.\n",
      "Epoch 02040 | Train Loss 1.2167 | Train Acc 0.6376 | Val Loss 1.2269 | Val Acc 0.6328\n",
      "Epoch 02060 | Train Loss 1.2167 | Train Acc 0.6378 | Val Loss 1.2281 | Val Acc 0.6334\n",
      "Epoch 02080 | Train Loss 1.2192 | Train Acc 0.6371 | Val Loss 1.2266 | Val Acc 0.6334\n",
      "Epoch  2093: reducing learning rate of group 0 to 2.3757e-04.\n",
      "Epoch 02100 | Train Loss 1.2174 | Train Acc 0.6371 | Val Loss 1.2233 | Val Acc 0.6326\n",
      "Epoch 02120 | Train Loss 1.2169 | Train Acc 0.6373 | Val Loss 1.2276 | Val Acc 0.6313\n",
      "Epoch 02140 | Train Loss 1.2144 | Train Acc 0.6371 | Val Loss 1.2233 | Val Acc 0.6338\n",
      "Epoch  2144: reducing learning rate of group 0 to 1.7818e-04.\n",
      "Epoch 02160 | Train Loss 1.2172 | Train Acc 0.6377 | Val Loss 1.2291 | Val Acc 0.6358\n",
      "Epoch 02180 | Train Loss 1.2194 | Train Acc 0.6372 | Val Loss 1.2267 | Val Acc 0.6326\n",
      "Epoch  2195: reducing learning rate of group 0 to 1.3363e-04.\n",
      "Epoch 02200 | Train Loss 1.2197 | Train Acc 0.6374 | Val Loss 1.2305 | Val Acc 0.6338\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_2200.pt'.\n",
      "Epoch 02220 | Train Loss 1.2196 | Train Acc 0.6361 | Val Loss 1.2300 | Val Acc 0.6316\n",
      "Epoch 02240 | Train Loss 1.2175 | Train Acc 0.6372 | Val Loss 1.2292 | Val Acc 0.6337\n",
      "Epoch  2246: reducing learning rate of group 0 to 1.0023e-04.\n",
      "Epoch 02260 | Train Loss 1.2153 | Train Acc 0.6373 | Val Loss 1.2245 | Val Acc 0.6331\n",
      "Epoch 02280 | Train Loss 1.2181 | Train Acc 0.6379 | Val Loss 1.2264 | Val Acc 0.6354\n",
      "Epoch  2297: reducing learning rate of group 0 to 7.5169e-05.\n",
      "Epoch 02300 | Train Loss 1.2165 | Train Acc 0.6381 | Val Loss 1.2225 | Val Acc 0.6356\n",
      "Epoch 02320 | Train Loss 1.2149 | Train Acc 0.6384 | Val Loss 1.2282 | Val Acc 0.6344\n",
      "Epoch 02340 | Train Loss 1.2182 | Train Acc 0.6372 | Val Loss 1.2254 | Val Acc 0.6329\n",
      "Epoch  2348: reducing learning rate of group 0 to 5.6377e-05.\n",
      "Epoch 02360 | Train Loss 1.2168 | Train Acc 0.6367 | Val Loss 1.2237 | Val Acc 0.6333\n",
      "Epoch 02380 | Train Loss 1.2162 | Train Acc 0.6384 | Val Loss 1.2239 | Val Acc 0.6334\n",
      "Epoch  2399: reducing learning rate of group 0 to 4.2283e-05.\n",
      "Epoch 02400 | Train Loss 1.2178 | Train Acc 0.6374 | Val Loss 1.2287 | Val Acc 0.6335\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_2400.pt'.\n",
      "Epoch 02420 | Train Loss 1.2160 | Train Acc 0.6371 | Val Loss 1.2250 | Val Acc 0.6335\n",
      "Epoch 02440 | Train Loss 1.2184 | Train Acc 0.6374 | Val Loss 1.2277 | Val Acc 0.6342\n",
      "Epoch  2450: reducing learning rate of group 0 to 3.1712e-05.\n",
      "Epoch 02460 | Train Loss 1.2187 | Train Acc 0.6379 | Val Loss 1.2301 | Val Acc 0.6341\n",
      "Epoch 02480 | Train Loss 1.2147 | Train Acc 0.6385 | Val Loss 1.2277 | Val Acc 0.6335\n",
      "Epoch  2501: reducing learning rate of group 0 to 2.3784e-05.\n",
      "Epoch 02500 | Train Loss 1.2201 | Train Acc 0.6366 | Val Loss 1.2295 | Val Acc 0.6326\n",
      "Epoch 02520 | Train Loss 1.2173 | Train Acc 0.6372 | Val Loss 1.2277 | Val Acc 0.6330\n",
      "Epoch 02540 | Train Loss 1.2196 | Train Acc 0.6378 | Val Loss 1.2297 | Val Acc 0.6334\n",
      "Epoch  2552: reducing learning rate of group 0 to 1.7838e-05.\n",
      "Epoch 02560 | Train Loss 1.2172 | Train Acc 0.6376 | Val Loss 1.2236 | Val Acc 0.6339\n",
      "Epoch 02580 | Train Loss 1.2179 | Train Acc 0.6381 | Val Loss 1.2261 | Val Acc 0.6332\n",
      "Epoch 02600 | Train Loss 1.2167 | Train Acc 0.6375 | Val Loss 1.2245 | Val Acc 0.6336\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_2600.pt'.\n",
      "Epoch  2603: reducing learning rate of group 0 to 1.3379e-05.\n",
      "Epoch 02620 | Train Loss 1.2187 | Train Acc 0.6377 | Val Loss 1.2266 | Val Acc 0.6337\n",
      "Epoch 02640 | Train Loss 1.2191 | Train Acc 0.6369 | Val Loss 1.2289 | Val Acc 0.6344\n",
      "Epoch  2654: reducing learning rate of group 0 to 1.0034e-05.\n",
      "Epoch 02660 | Train Loss 1.2175 | Train Acc 0.6369 | Val Loss 1.2295 | Val Acc 0.6337\n",
      "Epoch 02680 | Train Loss 1.2183 | Train Acc 0.6374 | Val Loss 1.2274 | Val Acc 0.6333\n",
      "Epoch 02700 | Train Loss 1.2174 | Train Acc 0.6370 | Val Loss 1.2246 | Val Acc 0.6338\n",
      "Epoch  2705: reducing learning rate of group 0 to 7.5254e-06.\n",
      "Epoch 02720 | Train Loss 1.2179 | Train Acc 0.6379 | Val Loss 1.2225 | Val Acc 0.6356\n",
      "Epoch 02740 | Train Loss 1.2168 | Train Acc 0.6377 | Val Loss 1.2250 | Val Acc 0.6343\n",
      "Epoch  2756: reducing learning rate of group 0 to 5.6441e-06.\n",
      "Epoch 02760 | Train Loss 1.2161 | Train Acc 0.6372 | Val Loss 1.2252 | Val Acc 0.6335\n",
      "Epoch 02780 | Train Loss 1.2164 | Train Acc 0.6380 | Val Loss 1.2234 | Val Acc 0.6369\n",
      "Epoch 02800 | Train Loss 1.2158 | Train Acc 0.6376 | Val Loss 1.2288 | Val Acc 0.6337\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_2800.pt'.\n",
      "Epoch  2807: reducing learning rate of group 0 to 4.2331e-06.\n",
      "Epoch 02820 | Train Loss 1.2142 | Train Acc 0.6391 | Val Loss 1.2213 | Val Acc 0.6348\n",
      "Epoch 02840 | Train Loss 1.2167 | Train Acc 0.6372 | Val Loss 1.2284 | Val Acc 0.6341\n",
      "Epoch  2858: reducing learning rate of group 0 to 3.1748e-06.\n",
      "Epoch 02860 | Train Loss 1.2179 | Train Acc 0.6376 | Val Loss 1.2255 | Val Acc 0.6322\n",
      "Epoch 02880 | Train Loss 1.2181 | Train Acc 0.6366 | Val Loss 1.2279 | Val Acc 0.6334\n",
      "Epoch 02900 | Train Loss 1.2181 | Train Acc 0.6372 | Val Loss 1.2274 | Val Acc 0.6328\n",
      "Epoch  2909: reducing learning rate of group 0 to 2.3811e-06.\n",
      "Epoch 02920 | Train Loss 1.2176 | Train Acc 0.6370 | Val Loss 1.2283 | Val Acc 0.6322\n",
      "Epoch 02940 | Train Loss 1.2156 | Train Acc 0.6382 | Val Loss 1.2267 | Val Acc 0.6330\n",
      "Epoch  2960: reducing learning rate of group 0 to 1.7858e-06.\n",
      "Epoch 02960 | Train Loss 1.2160 | Train Acc 0.6379 | Val Loss 1.2226 | Val Acc 0.6350\n",
      "Epoch 02980 | Train Loss 1.2197 | Train Acc 0.6368 | Val Loss 1.2252 | Val Acc 0.6335\n",
      "Epoch 03000 | Train Loss 1.2163 | Train Acc 0.6384 | Val Loss 1.2231 | Val Acc 0.6328\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_3000.pt'.\n",
      "Epoch  3011: reducing learning rate of group 0 to 1.3394e-06.\n",
      "Epoch 03020 | Train Loss 1.2147 | Train Acc 0.6384 | Val Loss 1.2212 | Val Acc 0.6350\n",
      "Epoch 03040 | Train Loss 1.2187 | Train Acc 0.6382 | Val Loss 1.2270 | Val Acc 0.6350\n",
      "Epoch 03060 | Train Loss 1.2169 | Train Acc 0.6375 | Val Loss 1.2321 | Val Acc 0.6332\n",
      "Epoch  3062: reducing learning rate of group 0 to 1.0045e-06.\n",
      "Epoch 03080 | Train Loss 1.2163 | Train Acc 0.6375 | Val Loss 1.2241 | Val Acc 0.6334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03100 | Train Loss 1.2170 | Train Acc 0.6377 | Val Loss 1.2238 | Val Acc 0.6334\n",
      "Epoch 03120 | Train Loss 1.2168 | Train Acc 0.6380 | Val Loss 1.2257 | Val Acc 0.6353\n",
      "Epoch  3133: reducing learning rate of group 0 to 7.5339e-07.\n",
      "Epoch 03140 | Train Loss 1.2183 | Train Acc 0.6369 | Val Loss 1.2291 | Val Acc 0.6340\n",
      "Epoch 03160 | Train Loss 1.2156 | Train Acc 0.6371 | Val Loss 1.2238 | Val Acc 0.6334\n",
      "Epoch 03180 | Train Loss 1.2173 | Train Acc 0.6371 | Val Loss 1.2266 | Val Acc 0.6339\n",
      "Epoch  3184: reducing learning rate of group 0 to 5.6504e-07.\n",
      "Epoch 03200 | Train Loss 1.2167 | Train Acc 0.6373 | Val Loss 1.2243 | Val Acc 0.6333\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_3200.pt'.\n",
      "Epoch 03220 | Train Loss 1.2168 | Train Acc 0.6382 | Val Loss 1.2235 | Val Acc 0.6336\n",
      "Epoch  3235: reducing learning rate of group 0 to 4.2378e-07.\n",
      "Epoch 03240 | Train Loss 1.2177 | Train Acc 0.6366 | Val Loss 1.2241 | Val Acc 0.6336\n",
      "Epoch 03260 | Train Loss 1.2166 | Train Acc 0.6373 | Val Loss 1.2260 | Val Acc 0.6336\n",
      "Epoch 03280 | Train Loss 1.2150 | Train Acc 0.6388 | Val Loss 1.2255 | Val Acc 0.6349\n",
      "Epoch  3286: reducing learning rate of group 0 to 3.1784e-07.\n",
      "Epoch 03300 | Train Loss 1.2176 | Train Acc 0.6373 | Val Loss 1.2246 | Val Acc 0.6333\n",
      "Epoch 03320 | Train Loss 1.2164 | Train Acc 0.6378 | Val Loss 1.2273 | Val Acc 0.6322\n",
      "Epoch  3337: reducing learning rate of group 0 to 2.3838e-07.\n",
      "Epoch 03340 | Train Loss 1.2175 | Train Acc 0.6367 | Val Loss 1.2239 | Val Acc 0.6343\n",
      "Epoch 03360 | Train Loss 1.2166 | Train Acc 0.6361 | Val Loss 1.2279 | Val Acc 0.6312\n",
      "Epoch 03380 | Train Loss 1.2184 | Train Acc 0.6369 | Val Loss 1.2316 | Val Acc 0.6328\n",
      "Epoch  3388: reducing learning rate of group 0 to 1.7878e-07.\n",
      "Epoch 03400 | Train Loss 1.2167 | Train Acc 0.6389 | Val Loss 1.2266 | Val Acc 0.6354\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_3400.pt'.\n",
      "Epoch 03420 | Train Loss 1.2157 | Train Acc 0.6367 | Val Loss 1.2285 | Val Acc 0.6321\n",
      "Epoch  3439: reducing learning rate of group 0 to 1.3409e-07.\n",
      "Epoch 03440 | Train Loss 1.2171 | Train Acc 0.6373 | Val Loss 1.2228 | Val Acc 0.6356\n",
      "Epoch 03460 | Train Loss 1.2177 | Train Acc 0.6385 | Val Loss 1.2245 | Val Acc 0.6367\n",
      "Epoch 03480 | Train Loss 1.2148 | Train Acc 0.6388 | Val Loss 1.2215 | Val Acc 0.6347\n",
      "Epoch  3490: reducing learning rate of group 0 to 1.0057e-07.\n",
      "Epoch 03500 | Train Loss 1.2162 | Train Acc 0.6376 | Val Loss 1.2301 | Val Acc 0.6312\n",
      "Epoch 03520 | Train Loss 1.2148 | Train Acc 0.6388 | Val Loss 1.2255 | Val Acc 0.6371\n",
      "Epoch  3541: reducing learning rate of group 0 to 7.5424e-08.\n",
      "Epoch 03540 | Train Loss 1.2168 | Train Acc 0.6378 | Val Loss 1.2282 | Val Acc 0.6348\n",
      "Epoch 03560 | Train Loss 1.2174 | Train Acc 0.6380 | Val Loss 1.2299 | Val Acc 0.6335\n",
      "Epoch 03580 | Train Loss 1.2196 | Train Acc 0.6372 | Val Loss 1.2260 | Val Acc 0.6342\n",
      "Epoch  3592: reducing learning rate of group 0 to 5.6568e-08.\n",
      "Epoch 03600 | Train Loss 1.2171 | Train Acc 0.6377 | Val Loss 1.2264 | Val Acc 0.6343\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_3600.pt'.\n",
      "Epoch 03620 | Train Loss 1.2166 | Train Acc 0.6385 | Val Loss 1.2253 | Val Acc 0.6366\n",
      "Epoch 03640 | Train Loss 1.2163 | Train Acc 0.6375 | Val Loss 1.2247 | Val Acc 0.6343\n",
      "Epoch  3643: reducing learning rate of group 0 to 4.2426e-08.\n",
      "Epoch 03660 | Train Loss 1.2176 | Train Acc 0.6372 | Val Loss 1.2229 | Val Acc 0.6337\n",
      "Epoch 03680 | Train Loss 1.2175 | Train Acc 0.6373 | Val Loss 1.2269 | Val Acc 0.6334\n",
      "Epoch  3694: reducing learning rate of group 0 to 3.1820e-08.\n",
      "Epoch 03700 | Train Loss 1.2170 | Train Acc 0.6372 | Val Loss 1.2269 | Val Acc 0.6344\n",
      "Epoch 03720 | Train Loss 1.2160 | Train Acc 0.6377 | Val Loss 1.2280 | Val Acc 0.6345\n",
      "Epoch 03740 | Train Loss 1.2189 | Train Acc 0.6367 | Val Loss 1.2300 | Val Acc 0.6341\n",
      "Epoch 03760 | Train Loss 1.2195 | Train Acc 0.6369 | Val Loss 1.2269 | Val Acc 0.6339\n",
      "Epoch 03780 | Train Loss 1.2188 | Train Acc 0.6372 | Val Loss 1.2303 | Val Acc 0.6317\n",
      "Epoch 03800 | Train Loss 1.2197 | Train Acc 0.6372 | Val Loss 1.2317 | Val Acc 0.6318\n",
      "Model saved in './saved_models/appnp_aminer/checkpoint_epoch_3800.pt'.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-038289edb51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m               \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m               \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m               verbose=True)\n\u001b[0m",
      "\u001b[0;32m~/research/grb/grb/model/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, n_epoch, save_dir, eval_every, save_every, dropout, verbose)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/grb/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(model=model,\n",
    "              n_epoch=5000,\n",
    "              save_dir=\"./saved_models/appnp_aminer\",\n",
    "              eval_every=20,\n",
    "              save_after=200,\n",
    "              dropout=0.5,\n",
    "              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f227a52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T06:01:16.481929Z",
     "start_time": "2021-05-06T06:01:16.475499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./saved_models/appnp_aminer/checkpoint_epoch_3800.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea3da9e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T06:01:17.061368Z",
     "start_time": "2021-05-06T06:01:16.930192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ACC: 0.6444\n"
     ]
    }
   ],
   "source": [
    "logits, test_acc = trainer.inference(model)\n",
    "print(\"Test ACC: {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b88ba6",
   "metadata": {},
   "source": [
    "## GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0788c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from grb.model.gat import GAT\n",
    "\n",
    "model = GAT(in_features=num_features, out_features=num_classes, hidden_features=128, alpha=0.01, k=10)\n",
    "print(\"Number of parameters: {}.\".format(get_num_params(model)))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157271ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
